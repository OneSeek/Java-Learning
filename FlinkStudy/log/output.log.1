13:28:28.579 [main] INFO  Main - Start process Main...
13:28:43.342 [main] INFO  cn.oneseek.KafkaDemo - 将kafka生产者发来的数据进行处理，本例子未进行任何处理
13:28:43.401 [main] WARN  o.a.f.s.c.k.FlinkKafkaProducer011 - Property [transaction.timeout.ms] not specified. Setting it to 3600000 ms
13:28:43.509 [main] INFO  o.a.f.s.a.e.LocalStreamEnvironment - Running job on local embedded Flink mini cluster
13:28:43.962 [main] INFO  o.a.f.r.minicluster.MiniCluster - Starting Flink Mini Cluster
13:28:43.965 [main] INFO  o.a.f.r.minicluster.MiniCluster - Starting Metrics Registry
13:28:44.025 [main] INFO  o.a.f.r.metrics.MetricRegistryImpl - No metrics reporter configured, no metrics will be exposed/reported.
13:28:44.028 [main] INFO  o.a.f.r.minicluster.MiniCluster - Starting RPC Service(s)
13:28:44.533 [flink-akka.actor.default-dispatcher-3] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
13:28:44.692 [main] INFO  o.a.f.r.rpc.akka.AkkaRpcServiceUtils - Trying to start actor system at :0
13:28:44.738 [flink-metrics-2] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
13:28:44.754 [flink-metrics-2] INFO  akka.remote.Remoting - Starting remoting
13:28:45.259 [flink-metrics-2] INFO  akka.remote.Remoting - Remoting started; listening on addresses :[akka.tcp://flink-metrics@10.80.74.184:64981]
13:28:45.286 [main] INFO  o.a.f.r.rpc.akka.AkkaRpcServiceUtils - Actor system started at akka.tcp://flink-metrics@10.80.74.184:64981
13:28:45.292 [main] INFO  o.a.f.r.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/MetricQueryService .
13:28:45.303 [main] INFO  o.a.f.r.minicluster.MiniCluster - Starting high-availability services
13:28:45.316 [main] INFO  o.a.flink.runtime.blob.BlobServer - Created BLOB server storage directory C:\Users\sx-9633\AppData\Local\Temp\blobStore-86d5337f-fdc3-425b-9a59-f96d4f5f905c
13:28:45.345 [main] INFO  o.a.flink.runtime.blob.BlobServer - Started BLOB server at 0.0.0.0:64982 - max concurrent requests: 50 - max backlog: 1000
13:28:45.349 [main] INFO  o.a.f.r.blob.PermanentBlobCache - Created BLOB cache storage directory C:\Users\sx-9633\AppData\Local\Temp\blobStore-23f2d45a-6239-41f4-880f-13a2210a2b88
13:28:45.350 [main] INFO  o.a.f.r.blob.TransientBlobCache - Created BLOB cache storage directory C:\Users\sx-9633\AppData\Local\Temp\blobStore-243df3c0-d8e1-4197-b83a-c59df84760bb
13:28:45.351 [main] INFO  o.a.f.r.minicluster.MiniCluster - Starting 1 TaskManger(s)
13:28:45.353 [main] INFO  o.a.f.r.t.TaskManagerRunner - Starting TaskManager with ResourceID: 54442bf5-a7e3-4974-87b1-720041d22e1b
13:28:45.482 [main] INFO  o.a.f.r.t.TaskManagerServices - Temporary file directory 'C:\Users\sx-9633\AppData\Local\Temp': total 237 GB, usable 160 GB (67.51% usable)
13:28:45.486 [main] INFO  o.a.f.r.i.d.FileChannelManagerImpl - FileChannelManager uses directory C:\Users\sx-9633\AppData\Local\Temp\flink-io-6a91c167-2ae1-409b-805c-1289bbd448f4 for spill files.
13:28:45.496 [main] INFO  o.a.f.r.i.d.FileChannelManagerImpl - FileChannelManager uses directory C:\Users\sx-9633\AppData\Local\Temp\flink-netty-shuffle-0fc673c2-7899-4a8a-af2a-7daea8da8e8e for spill files.
13:28:45.658 [main] INFO  o.a.f.r.i.n.buffer.NetworkBufferPool - Allocated 402 MB for network buffer pool (number of memory segments: 12876, bytes per segment: 32768).
13:28:45.664 [main] INFO  o.a.f.r.i.n.NettyShuffleEnvironment - Starting the network environment and its components.
13:28:45.665 [main] INFO  o.a.f.r.taskexecutor.KvStateService - Starting the kvState service and its components.
13:28:45.665 [main] INFO  o.a.f.r.t.TaskManagerServices - Limiting managed memory to 0.7 of the currently free heap space (2526 MB), memory will be allocated lazily.
13:28:45.676 [main] INFO  o.a.f.r.t.TaskManagerConfiguration - Messages have a max timeout of 10000 ms
13:28:45.687 [main] INFO  o.a.f.r.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/taskmanager_0 .
13:28:45.699 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.t.JobLeaderService - Start job leader service.
13:28:45.701 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.runtime.filecache.FileCache - User file cache uses directory C:\Users\sx-9633\AppData\Local\Temp\flink-dist-cache-ec25d13f-c943-4e23-9a63-b07cc8c4aff2
13:28:45.743 [main] INFO  o.a.f.r.d.DispatcherRestEndpoint - Starting rest endpoint.
13:28:45.958 [main] WARN  o.a.f.r.webmonitor.WebMonitorUtils - Log file environment variable 'log.file' is not set.
13:28:45.958 [main] WARN  o.a.f.r.webmonitor.WebMonitorUtils - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'Key: 'web.log.path' , default: null (fallback keys: [{key=jobmanager.web.log.path, isDeprecated=true}])'.
13:28:45.968 [main] INFO  o.a.f.r.d.DispatcherRestEndpoint - Failed to load web based job submission extension. Probable reason: flink-runtime-web is not in the classpath.
13:28:46.773 [main] INFO  o.a.f.r.d.DispatcherRestEndpoint - Rest endpoint listening at localhost:65002
13:28:46.774 [main] INFO  o.a.f.r.h.n.e.EmbeddedLeaderService - Proposing leadership to contender org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint@6ae7deac @ http://localhost:65002
13:28:46.775 [mini-cluster-io-thread-1] INFO  o.a.f.r.d.DispatcherRestEndpoint - http://localhost:65002 was granted leadership with leaderSessionID=04fcfa14-5995-4efd-81f7-e351c13125d3
13:28:46.776 [mini-cluster-io-thread-1] INFO  o.a.f.r.h.n.e.EmbeddedLeaderService - Received confirmation of leadership for leader http://localhost:65002 , session=04fcfa14-5995-4efd-81f7-e351c13125d3
13:28:46.786 [main] INFO  o.a.f.r.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/resourcemanager .
13:28:46.796 [main] INFO  o.a.f.r.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/dispatcher .
13:28:46.805 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.h.n.e.EmbeddedLeaderService - Proposing leadership to contender org.apache.flink.runtime.resourcemanager.StandaloneResourceManager@5c438979 @ akka://flink/user/resourcemanager
13:28:46.806 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.h.n.e.EmbeddedLeaderService - Proposing leadership to contender org.apache.flink.runtime.dispatcher.StandaloneDispatcher@5806084f @ akka://flink/user/dispatcher
13:28:46.807 [main] INFO  o.a.f.r.minicluster.MiniCluster - Flink Mini Cluster started successfully
13:28:46.807 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.r.StandaloneResourceManager - ResourceManager akka://flink/user/resourcemanager was granted leadership with fencing token 948566be7f91c4fd422f77912f3b4854
13:28:46.810 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.r.s.SlotManagerImpl - Starting the SlotManager.
13:28:46.814 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.d.StandaloneDispatcher - Dispatcher akka://flink/user/dispatcher was granted leadership with fencing token 9d471afe-f31f-46c2-898f-cdbb4b6b33bc
13:28:46.816 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.d.StandaloneDispatcher - Recovering all persisted jobs.
13:28:46.813 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.h.n.e.EmbeddedLeaderService - Received confirmation of leadership for leader akka://flink/user/resourcemanager , session=422f7791-2f3b-4854-9485-66be7f91c4fd
13:28:46.817 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.h.n.e.EmbeddedLeaderService - Received confirmation of leadership for leader akka://flink/user/dispatcher , session=9d471afe-f31f-46c2-898f-cdbb4b6b33bc
13:28:46.820 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.taskexecutor.TaskExecutor - Connecting to ResourceManager akka://flink/user/resourcemanager(948566be7f91c4fd422f77912f3b4854).
13:28:46.828 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.taskexecutor.TaskExecutor - Resolved ResourceManager address, beginning registration
13:28:46.828 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.taskexecutor.TaskExecutor - Registration at ResourceManager attempt 1 (timeout=100ms)
13:28:46.837 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.r.StandaloneResourceManager - Registering TaskManager with ResourceID 54442bf5-a7e3-4974-87b1-720041d22e1b (akka://flink/user/taskmanager_0) at ResourceManager
13:28:46.838 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.d.StandaloneDispatcher - Received JobGraph submission b3f282625c3b53b9fb1fd47333b8b776 (Flink Streaming Java API Skeleton).
13:28:46.839 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.d.StandaloneDispatcher - Submitting job b3f282625c3b53b9fb1fd47333b8b776 (Flink Streaming Java API Skeleton).
13:28:46.839 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.taskexecutor.TaskExecutor - Successful registration at resource manager akka://flink/user/resourcemanager under registration id 35031a1f8ee162490900523902332a27.
13:28:46.857 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/jobmanager_1 .
13:28:46.864 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.runtime.jobmaster.JobMaster - Initializing job Flink Streaming Java API Skeleton (b3f282625c3b53b9fb1fd47333b8b776).
13:28:46.878 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.runtime.jobmaster.JobMaster - Using restart strategy FixedDelayRestartStrategy(maxNumberRestartAttempts=2147483647, delayBetweenRestartAttempts=0) for Flink Streaming Java API Skeleton (b3f282625c3b53b9fb1fd47333b8b776).
13:28:46.893 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.e.ExecutionGraph - Job recovers via failover strategy: full graph restart
13:28:46.908 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.runtime.jobmaster.JobMaster - Running initialization on master for job Flink Streaming Java API Skeleton (b3f282625c3b53b9fb1fd47333b8b776).
13:28:46.908 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.runtime.jobmaster.JobMaster - Successfully ran initialization on master in 0 ms.
13:28:46.936 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.runtime.jobmaster.JobMaster - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
13:28:46.946 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.h.n.e.EmbeddedLeaderService - Proposing leadership to contender org.apache.flink.runtime.jobmaster.JobManagerRunner@6be0de13 @ akka://flink/user/jobmanager_1
13:28:46.947 [mini-cluster-io-thread-4] INFO  o.a.f.r.jobmaster.JobManagerRunner - JobManager runner for job Flink Streaming Java API Skeleton (b3f282625c3b53b9fb1fd47333b8b776) was granted leadership with session id 4069f4e0-028b-42a8-a9e2-67ff100edce9 at akka://flink/user/jobmanager_1.
13:28:46.950 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.runtime.jobmaster.JobMaster - Starting execution of job Flink Streaming Java API Skeleton (b3f282625c3b53b9fb1fd47333b8b776) under job master id a9e267ff100edce94069f4e0028b42a8.
13:28:46.950 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Job Flink Streaming Java API Skeleton (b3f282625c3b53b9fb1fd47333b8b776) switched from state CREATED to RUNNING.
13:28:46.954 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (c65f36f996e317b1210db9b837a45054) switched from CREATED to SCHEDULED.
13:28:46.963 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.j.slotpool.SlotPoolImpl - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{5ea75fd9324e69bbd806f602bff6e727}]
13:28:46.968 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (3905610f3a26e292f2eafc50bef6fc8f) switched from CREATED to SCHEDULED.
13:28:46.969 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.j.slotpool.SlotPoolImpl - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{cc4223c559a085d8b34cb69aea1f8796}]
13:28:46.969 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (3208b0176ab22c8465e6f37befb2ab2c) switched from CREATED to SCHEDULED.
13:28:46.969 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.j.slotpool.SlotPoolImpl - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{e19ba280b369a45306098554e0953e1f}]
13:28:46.969 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (a94c982a42653a98304ebf712141d472) switched from CREATED to SCHEDULED.
13:28:46.969 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.j.slotpool.SlotPoolImpl - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{e52bc973dc995bb13b44633fb2048628}]
13:28:46.970 [jobmanager-future-thread-1] INFO  o.a.f.r.h.n.e.EmbeddedLeaderService - Received confirmation of leadership for leader akka://flink/user/jobmanager_1 , session=4069f4e0-028b-42a8-a9e2-67ff100edce9
13:28:46.971 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.runtime.jobmaster.JobMaster - Connecting to ResourceManager akka://flink/user/resourcemanager(948566be7f91c4fd422f77912f3b4854)
13:28:46.972 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.runtime.jobmaster.JobMaster - Resolved ResourceManager address, beginning registration
13:28:46.972 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.runtime.jobmaster.JobMaster - Registration at ResourceManager attempt 1 (timeout=100ms)
13:28:46.974 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.r.StandaloneResourceManager - Registering job manager a9e267ff100edce94069f4e0028b42a8@akka://flink/user/jobmanager_1 for job b3f282625c3b53b9fb1fd47333b8b776.
13:28:46.977 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.r.StandaloneResourceManager - Registered job manager a9e267ff100edce94069f4e0028b42a8@akka://flink/user/jobmanager_1 for job b3f282625c3b53b9fb1fd47333b8b776.
13:28:46.979 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.runtime.jobmaster.JobMaster - JobManager successfully registered at ResourceManager, leader id: 948566be7f91c4fd422f77912f3b4854.
13:28:46.979 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.j.slotpool.SlotPoolImpl - Requesting new slot [SlotRequestId{5ea75fd9324e69bbd806f602bff6e727}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} from resource manager.
13:28:46.980 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.r.StandaloneResourceManager - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} for job b3f282625c3b53b9fb1fd47333b8b776 with allocation id 59701bd3fd22bfee1229f09f2cb7ad83.
13:28:46.981 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.j.slotpool.SlotPoolImpl - Requesting new slot [SlotRequestId{cc4223c559a085d8b34cb69aea1f8796}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} from resource manager.
13:28:46.982 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.taskexecutor.TaskExecutor - Receive slot request 59701bd3fd22bfee1229f09f2cb7ad83 for job b3f282625c3b53b9fb1fd47333b8b776 from resource manager with leader id 948566be7f91c4fd422f77912f3b4854.
13:28:46.982 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.r.StandaloneResourceManager - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} for job b3f282625c3b53b9fb1fd47333b8b776 with allocation id a18a859b63d5ac657237c616d2467d14.
13:28:46.983 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.taskexecutor.TaskExecutor - Allocated slot for 59701bd3fd22bfee1229f09f2cb7ad83.
13:28:46.982 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.j.slotpool.SlotPoolImpl - Requesting new slot [SlotRequestId{e19ba280b369a45306098554e0953e1f}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} from resource manager.
13:28:46.983 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.t.JobLeaderService - Add job b3f282625c3b53b9fb1fd47333b8b776 for job leader monitoring.
13:28:46.983 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.j.slotpool.SlotPoolImpl - Requesting new slot [SlotRequestId{e52bc973dc995bb13b44633fb2048628}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} from resource manager.
13:28:46.984 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.r.StandaloneResourceManager - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} for job b3f282625c3b53b9fb1fd47333b8b776 with allocation id da78430eb15052bb3688c104371a71b7.
13:28:46.984 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.taskexecutor.TaskExecutor - Receive slot request a18a859b63d5ac657237c616d2467d14 for job b3f282625c3b53b9fb1fd47333b8b776 from resource manager with leader id 948566be7f91c4fd422f77912f3b4854.
13:28:46.984 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.r.StandaloneResourceManager - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} for job b3f282625c3b53b9fb1fd47333b8b776 with allocation id da99d1ad714fcf930e90c3a4ee1e3766.
13:28:46.985 [mini-cluster-io-thread-2] INFO  o.a.f.r.t.JobLeaderService - Try to register at job manager akka://flink/user/jobmanager_1 with leader id 4069f4e0-028b-42a8-a9e2-67ff100edce9.
13:28:46.984 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.taskexecutor.TaskExecutor - Allocated slot for a18a859b63d5ac657237c616d2467d14.
13:28:46.985 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.t.JobLeaderService - Add job b3f282625c3b53b9fb1fd47333b8b776 for job leader monitoring.
13:28:46.985 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.taskexecutor.TaskExecutor - Receive slot request da78430eb15052bb3688c104371a71b7 for job b3f282625c3b53b9fb1fd47333b8b776 from resource manager with leader id 948566be7f91c4fd422f77912f3b4854.
13:28:46.985 [mini-cluster-io-thread-2] INFO  o.a.f.r.t.JobLeaderService - Try to register at job manager akka://flink/user/jobmanager_1 with leader id 4069f4e0-028b-42a8-a9e2-67ff100edce9.
13:28:46.986 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.taskexecutor.TaskExecutor - Allocated slot for da78430eb15052bb3688c104371a71b7.
13:28:46.986 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.t.JobLeaderService - Resolved JobManager address, beginning registration
13:28:46.986 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.t.JobLeaderService - Add job b3f282625c3b53b9fb1fd47333b8b776 for job leader monitoring.
13:28:46.987 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.t.JobLeaderService - Resolved JobManager address, beginning registration
13:28:46.987 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.taskexecutor.TaskExecutor - Receive slot request da99d1ad714fcf930e90c3a4ee1e3766 for job b3f282625c3b53b9fb1fd47333b8b776 from resource manager with leader id 948566be7f91c4fd422f77912f3b4854.
13:28:46.987 [mini-cluster-io-thread-3] INFO  o.a.f.r.t.JobLeaderService - Try to register at job manager akka://flink/user/jobmanager_1 with leader id 4069f4e0-028b-42a8-a9e2-67ff100edce9.
13:28:46.987 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.taskexecutor.TaskExecutor - Allocated slot for da99d1ad714fcf930e90c3a4ee1e3766.
13:28:46.987 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.t.JobLeaderService - Add job b3f282625c3b53b9fb1fd47333b8b776 for job leader monitoring.
13:28:46.987 [mini-cluster-io-thread-3] INFO  o.a.f.r.t.JobLeaderService - Try to register at job manager akka://flink/user/jobmanager_1 with leader id 4069f4e0-028b-42a8-a9e2-67ff100edce9.
13:28:46.987 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.t.JobLeaderService - Resolved JobManager address, beginning registration
13:28:46.988 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.t.JobLeaderService - Resolved JobManager address, beginning registration
13:28:46.988 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.t.JobLeaderService - Registration at JobManager attempt 1 (timeout=100ms)
13:28:46.989 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.t.JobLeaderService - Successful registration at job manager akka://flink/user/jobmanager_1 for job b3f282625c3b53b9fb1fd47333b8b776.
13:28:46.990 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.taskexecutor.TaskExecutor - Establish JobManager connection for job b3f282625c3b53b9fb1fd47333b8b776.
13:28:46.993 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.taskexecutor.TaskExecutor - Offer reserved slots to the leader of job b3f282625c3b53b9fb1fd47333b8b776.
13:28:46.999 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (c65f36f996e317b1210db9b837a45054) switched from SCHEDULED to DEPLOYING.
13:28:46.999 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Deploying Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (attempt #0) to 54442bf5-a7e3-4974-87b1-720041d22e1b @ activate.navicat.com (dataPort=-1)
13:28:47.002 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (3905610f3a26e292f2eafc50bef6fc8f) switched from SCHEDULED to DEPLOYING.
13:28:47.003 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Deploying Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (attempt #0) to 54442bf5-a7e3-4974-87b1-720041d22e1b @ activate.navicat.com (dataPort=-1)
13:28:47.003 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (3208b0176ab22c8465e6f37befb2ab2c) switched from SCHEDULED to DEPLOYING.
13:28:47.003 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Deploying Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (attempt #0) to 54442bf5-a7e3-4974-87b1-720041d22e1b @ activate.navicat.com (dataPort=-1)
13:28:47.003 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (a94c982a42653a98304ebf712141d472) switched from SCHEDULED to DEPLOYING.
13:28:47.003 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Deploying Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (attempt #0) to 54442bf5-a7e3-4974-87b1-720041d22e1b @ activate.navicat.com (dataPort=-1)
13:28:47.020 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.taskexecutor.TaskExecutor - Received task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4).
13:28:47.021 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.flink.runtime.taskmanager.Task - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (c65f36f996e317b1210db9b837a45054) switched from CREATED to DEPLOYING.
13:28:47.021 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.flink.runtime.taskmanager.Task - Creating FileSystem stream leak safety net for task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (c65f36f996e317b1210db9b837a45054) [DEPLOYING]
13:28:47.025 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (c65f36f996e317b1210db9b837a45054) [DEPLOYING].
13:28:47.026 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.flink.runtime.taskmanager.Task - Registering task at network: Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (c65f36f996e317b1210db9b837a45054) [DEPLOYING].
13:28:47.028 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.taskexecutor.TaskExecutor - Received task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4).
13:28:47.028 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.flink.runtime.taskmanager.Task - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (3905610f3a26e292f2eafc50bef6fc8f) switched from CREATED to DEPLOYING.
13:28:47.028 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.flink.runtime.taskmanager.Task - Creating FileSystem stream leak safety net for task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (3905610f3a26e292f2eafc50bef6fc8f) [DEPLOYING]
13:28:47.030 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (3905610f3a26e292f2eafc50bef6fc8f) [DEPLOYING].
13:28:47.031 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.taskexecutor.TaskExecutor - Received task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4).
13:28:47.032 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.flink.runtime.taskmanager.Task - Registering task at network: Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (3905610f3a26e292f2eafc50bef6fc8f) [DEPLOYING].
13:28:47.035 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.flink.runtime.taskmanager.Task - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (3208b0176ab22c8465e6f37befb2ab2c) switched from CREATED to DEPLOYING.
13:28:47.035 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.flink.runtime.taskmanager.Task - Creating FileSystem stream leak safety net for task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (3208b0176ab22c8465e6f37befb2ab2c) [DEPLOYING]
13:28:47.035 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (3208b0176ab22c8465e6f37befb2ab2c) [DEPLOYING].
13:28:47.036 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.flink.runtime.taskmanager.Task - Registering task at network: Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (3208b0176ab22c8465e6f37befb2ab2c) [DEPLOYING].
13:28:47.037 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.flink.runtime.taskmanager.Task - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (c65f36f996e317b1210db9b837a45054) switched from DEPLOYING to RUNNING.
13:28:47.038 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.flink.runtime.taskmanager.Task - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (3905610f3a26e292f2eafc50bef6fc8f) switched from DEPLOYING to RUNNING.
13:28:47.038 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.flink.runtime.taskmanager.Task - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (3208b0176ab22c8465e6f37befb2ab2c) switched from DEPLOYING to RUNNING.
13:28:47.038 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (c65f36f996e317b1210db9b837a45054) switched from DEPLOYING to RUNNING.
13:28:47.039 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (3905610f3a26e292f2eafc50bef6fc8f) switched from DEPLOYING to RUNNING.
13:28:47.039 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (3208b0176ab22c8465e6f37befb2ab2c) switched from DEPLOYING to RUNNING.
13:28:47.040 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.taskexecutor.TaskExecutor - Received task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4).
13:28:47.040 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.runtime.tasks.StreamTask - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
13:28:47.040 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.runtime.tasks.StreamTask - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
13:28:47.040 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.runtime.tasks.StreamTask - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
13:28:47.040 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.t.slot.TaskSlotTable - Activate slot a18a859b63d5ac657237c616d2467d14.
13:28:47.040 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.flink.runtime.taskmanager.Task - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (a94c982a42653a98304ebf712141d472) switched from CREATED to DEPLOYING.
13:28:47.040 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.flink.runtime.taskmanager.Task - Creating FileSystem stream leak safety net for task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (a94c982a42653a98304ebf712141d472) [DEPLOYING]
13:28:47.040 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.t.slot.TaskSlotTable - Activate slot da78430eb15052bb3688c104371a71b7.
13:28:47.041 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.t.slot.TaskSlotTable - Activate slot da99d1ad714fcf930e90c3a4ee1e3766.
13:28:47.041 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.t.slot.TaskSlotTable - Activate slot 59701bd3fd22bfee1229f09f2cb7ad83.
13:28:47.040 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (a94c982a42653a98304ebf712141d472) [DEPLOYING].
13:28:47.041 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.flink.runtime.taskmanager.Task - Registering task at network: Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (a94c982a42653a98304ebf712141d472) [DEPLOYING].
13:28:47.043 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.flink.runtime.taskmanager.Task - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (a94c982a42653a98304ebf712141d472) switched from DEPLOYING to RUNNING.
13:28:47.043 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.runtime.tasks.StreamTask - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
13:28:47.044 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (a94c982a42653a98304ebf712141d472) switched from DEPLOYING to RUNNING.
13:28:47.104 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - no state to restore
13:28:47.104 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - no state to restore
13:28:47.104 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - no state to restore
13:28:47.104 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - no state to restore
13:28:47.122 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:28:47.122 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:28:47.122 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:28:47.122 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13:28:47.219 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
13:28:47.219 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
13:28:47.221 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
13:28:47.221 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
13:28:47.222 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.c.k.FlinkKafkaProducer011 - Starting FlinkKafkaProducer (3/4) to produce into default topic test1
13:28:47.223 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.c.k.FlinkKafkaProducer011 - Starting FlinkKafkaProducer (2/4) to produce into default topic test1
13:28:47.223 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
13:28:47.223 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
13:28:47.223 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.c.k.FlinkKafkaProducer011 - Starting FlinkKafkaProducer (1/4) to produce into default topic test1
13:28:47.227 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.a.java.typeutils.TypeExtractor - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
13:28:47.227 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.a.java.typeutils.TypeExtractor - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
13:28:47.227 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.a.java.typeutils.TypeExtractor - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
13:28:47.228 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.a.java.typeutils.TypeExtractor - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
13:28:47.228 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.a.java.typeutils.TypeExtractor - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
13:28:47.230 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.a.java.typeutils.TypeExtractor - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
13:28:47.238 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 1 has no restore state.
13:28:47.238 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 has no restore state.
13:28:47.242 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 0 has no restore state.
13:28:47.244 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
13:28:47.244 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
13:28:47.244 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.c.k.FlinkKafkaProducer011 - Starting FlinkKafkaProducer (4/4) to produce into default topic test1
13:28:47.245 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.a.java.typeutils.TypeExtractor - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
13:28:47.245 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.a.java.typeutils.TypeExtractor - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
13:28:47.247 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 3 has no restore state.
13:28:47.247 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:28:47.248 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:28:47.249 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:28:47.250 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

13:28:47.310 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] WARN  o.a.k.c.consumer.ConsumerConfig - The configuration 'zookeeper.connect' was supplied but isn't a known config.
13:28:47.310 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.k.c.consumer.ConsumerConfig - The configuration 'zookeeper.connect' was supplied but isn't a known config.
13:28:47.310 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] WARN  o.a.k.c.consumer.ConsumerConfig - The configuration 'zookeeper.connect' was supplied but isn't a known config.
13:28:47.310 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] WARN  o.a.k.c.consumer.ConsumerConfig - The configuration 'zookeeper.connect' was supplied but isn't a known config.
13:28:47.310 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
13:28:47.310 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
13:28:47.310 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
13:28:47.310 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
13:28:47.310 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
13:28:47.310 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
13:28:47.310 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
13:28:47.311 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
13:28:48.439 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 1 @ 1595827728432 for job b3f282625c3b53b9fb1fd47333b8b776.
13:28:49.322 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:49.325 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:49.323 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:49.323 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:51.461 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:51.461 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:51.462 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:51.462 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:53.563 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:53.566 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:53.568 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:53.568 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:55.669 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:55.669 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:55.671 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:55.671 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:57.772 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:57.772 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:57.774 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:57.775 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:59.877 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:59.879 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:59.882 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:28:59.883 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:29:01.981 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:29:01.982 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:29:01.988 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
13:29:01.988 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] WARN  o.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
09:10:11.419 [main] INFO  cn.oneseek.KafkaDemo - 将kafka生产者发来的数据进行处理，本例子未进行任何处理
09:10:11.478 [main] WARN  o.a.f.s.c.k.FlinkKafkaProducer011 - Property [transaction.timeout.ms] not specified. Setting it to 3600000 ms
09:10:11.597 [main] INFO  o.a.f.s.a.e.LocalStreamEnvironment - Running job on local embedded Flink mini cluster
09:10:11.827 [main] INFO  o.a.f.r.minicluster.MiniCluster - Starting Flink Mini Cluster
09:10:11.830 [main] INFO  o.a.f.r.minicluster.MiniCluster - Starting Metrics Registry
09:10:11.920 [main] INFO  o.a.f.r.metrics.MetricRegistryImpl - No metrics reporter configured, no metrics will be exposed/reported.
09:10:11.920 [main] INFO  o.a.f.r.minicluster.MiniCluster - Starting RPC Service(s)
09:10:12.322 [flink-akka.actor.default-dispatcher-2] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
09:10:12.483 [main] INFO  o.a.f.r.rpc.akka.AkkaRpcServiceUtils - Trying to start actor system at :0
09:10:12.543 [flink-metrics-2] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
09:10:12.558 [flink-metrics-2] INFO  akka.remote.Remoting - Starting remoting
09:10:13.081 [main] INFO  o.a.f.r.rpc.akka.AkkaRpcServiceUtils - Actor system started at akka.tcp://flink-metrics@10.80.74.184:56691
09:10:13.084 [flink-metrics-2] INFO  akka.remote.Remoting - Remoting started; listening on addresses :[akka.tcp://flink-metrics@10.80.74.184:56691]
09:10:13.087 [main] INFO  o.a.f.r.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/MetricQueryService .
09:10:13.098 [main] INFO  o.a.f.r.minicluster.MiniCluster - Starting high-availability services
09:10:13.112 [main] INFO  o.a.flink.runtime.blob.BlobServer - Created BLOB server storage directory C:\Users\sx-9633\AppData\Local\Temp\blobStore-a71fb98b-b516-4178-9683-cba232139b6b
09:10:13.150 [main] INFO  o.a.flink.runtime.blob.BlobServer - Started BLOB server at 0.0.0.0:56692 - max concurrent requests: 50 - max backlog: 1000
09:10:13.154 [main] INFO  o.a.f.r.blob.PermanentBlobCache - Created BLOB cache storage directory C:\Users\sx-9633\AppData\Local\Temp\blobStore-b05f96be-de32-40bf-aee8-ec45214417ad
09:10:13.156 [main] INFO  o.a.f.r.blob.TransientBlobCache - Created BLOB cache storage directory C:\Users\sx-9633\AppData\Local\Temp\blobStore-a9c164bd-fdc3-4378-9494-5938a61a7da6
09:10:13.156 [main] INFO  o.a.f.r.minicluster.MiniCluster - Starting 1 TaskManger(s)
09:10:13.158 [main] INFO  o.a.f.r.t.TaskManagerRunner - Starting TaskManager with ResourceID: 437bf4a2-7385-435f-98b8-b4a64625e40f
09:10:13.274 [main] INFO  o.a.f.r.t.TaskManagerServices - Temporary file directory 'C:\Users\sx-9633\AppData\Local\Temp': total 237 GB, usable 154 GB (64.98% usable)
09:10:13.278 [main] INFO  o.a.f.r.i.d.FileChannelManagerImpl - FileChannelManager uses directory C:\Users\sx-9633\AppData\Local\Temp\flink-io-0edc9a23-0637-43e3-81ce-db37f01f326b for spill files.
09:10:13.288 [main] INFO  o.a.f.r.i.d.FileChannelManagerImpl - FileChannelManager uses directory C:\Users\sx-9633\AppData\Local\Temp\flink-netty-shuffle-c6ca5d0a-d953-4e11-bc9d-fa5c0915e915 for spill files.
09:10:13.459 [main] INFO  o.a.f.r.i.n.buffer.NetworkBufferPool - Allocated 402 MB for network buffer pool (number of memory segments: 12876, bytes per segment: 32768).
09:10:13.466 [main] INFO  o.a.f.r.i.n.NettyShuffleEnvironment - Starting the network environment and its components.
09:10:13.467 [main] INFO  o.a.f.r.taskexecutor.KvStateService - Starting the kvState service and its components.
09:10:13.468 [main] INFO  o.a.f.r.t.TaskManagerServices - Limiting managed memory to 0.7 of the currently free heap space (2527 MB), memory will be allocated lazily.
09:10:13.479 [main] INFO  o.a.f.r.t.TaskManagerConfiguration - Messages have a max timeout of 10000 ms
09:10:13.489 [main] INFO  o.a.f.r.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/taskmanager_0 .
09:10:13.506 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.t.JobLeaderService - Start job leader service.
09:10:13.508 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.runtime.filecache.FileCache - User file cache uses directory C:\Users\sx-9633\AppData\Local\Temp\flink-dist-cache-ccc777c6-5994-4a3b-9076-112d72773e9d
09:10:13.551 [main] INFO  o.a.f.r.d.DispatcherRestEndpoint - Starting rest endpoint.
09:10:13.748 [main] WARN  o.a.f.r.webmonitor.WebMonitorUtils - Log file environment variable 'log.file' is not set.
09:10:13.748 [main] WARN  o.a.f.r.webmonitor.WebMonitorUtils - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'Key: 'web.log.path' , default: null (fallback keys: [{key=jobmanager.web.log.path, isDeprecated=true}])'.
09:10:13.758 [main] INFO  o.a.f.r.d.DispatcherRestEndpoint - Failed to load web based job submission extension. Probable reason: flink-runtime-web is not in the classpath.
09:10:14.529 [main] INFO  o.a.f.r.d.DispatcherRestEndpoint - Rest endpoint listening at localhost:56711
09:10:14.530 [main] INFO  o.a.f.r.h.n.e.EmbeddedLeaderService - Proposing leadership to contender org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint@73ad4ecc @ http://localhost:56711
09:10:14.532 [mini-cluster-io-thread-1] INFO  o.a.f.r.d.DispatcherRestEndpoint - http://localhost:56711 was granted leadership with leaderSessionID=ffdb0c1b-3fa5-448f-8bec-d64ecc8bb047
09:10:14.532 [mini-cluster-io-thread-1] INFO  o.a.f.r.h.n.e.EmbeddedLeaderService - Received confirmation of leadership for leader http://localhost:56711 , session=ffdb0c1b-3fa5-448f-8bec-d64ecc8bb047
09:10:14.541 [main] INFO  o.a.f.r.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/resourcemanager .
09:10:14.552 [main] INFO  o.a.f.r.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/dispatcher .
09:10:14.564 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.h.n.e.EmbeddedLeaderService - Proposing leadership to contender org.apache.flink.runtime.resourcemanager.StandaloneResourceManager@49689046 @ akka://flink/user/resourcemanager
09:10:14.565 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.h.n.e.EmbeddedLeaderService - Proposing leadership to contender org.apache.flink.runtime.dispatcher.StandaloneDispatcher@a6776c2 @ akka://flink/user/dispatcher
09:10:14.567 [main] INFO  o.a.f.r.minicluster.MiniCluster - Flink Mini Cluster started successfully
09:10:14.567 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.r.StandaloneResourceManager - ResourceManager akka://flink/user/resourcemanager was granted leadership with fencing token ac49e46180780b710e3ac06a08334eca
09:10:14.571 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.r.s.SlotManagerImpl - Starting the SlotManager.
09:10:14.571 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.d.StandaloneDispatcher - Dispatcher akka://flink/user/dispatcher was granted leadership with fencing token 17ed91ea-79ea-4c72-88bc-ad27d3813d3b
09:10:14.572 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.d.StandaloneDispatcher - Recovering all persisted jobs.
09:10:14.573 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.h.n.e.EmbeddedLeaderService - Received confirmation of leadership for leader akka://flink/user/resourcemanager , session=0e3ac06a-0833-4eca-ac49-e46180780b71
09:10:14.576 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.h.n.e.EmbeddedLeaderService - Received confirmation of leadership for leader akka://flink/user/dispatcher , session=17ed91ea-79ea-4c72-88bc-ad27d3813d3b
09:10:14.578 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.taskexecutor.TaskExecutor - Connecting to ResourceManager akka://flink/user/resourcemanager(ac49e46180780b710e3ac06a08334eca).
09:10:14.586 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.taskexecutor.TaskExecutor - Resolved ResourceManager address, beginning registration
09:10:14.586 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.taskexecutor.TaskExecutor - Registration at ResourceManager attempt 1 (timeout=100ms)
09:10:14.594 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.r.StandaloneResourceManager - Registering TaskManager with ResourceID 437bf4a2-7385-435f-98b8-b4a64625e40f (akka://flink/user/taskmanager_0) at ResourceManager
09:10:14.596 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.taskexecutor.TaskExecutor - Successful registration at resource manager akka://flink/user/resourcemanager under registration id de2e2d0470cac7c1020e4246a5d45b48.
09:10:14.598 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.d.StandaloneDispatcher - Received JobGraph submission f4e075b4f58efd9c9f7f06e943b4e443 (Flink Streaming Java API Skeleton).
09:10:14.599 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.d.StandaloneDispatcher - Submitting job f4e075b4f58efd9c9f7f06e943b4e443 (Flink Streaming Java API Skeleton).
09:10:14.616 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/jobmanager_1 .
09:10:14.623 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.runtime.jobmaster.JobMaster - Initializing job Flink Streaming Java API Skeleton (f4e075b4f58efd9c9f7f06e943b4e443).
09:10:14.637 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.runtime.jobmaster.JobMaster - Using restart strategy FixedDelayRestartStrategy(maxNumberRestartAttempts=2147483647, delayBetweenRestartAttempts=0) for Flink Streaming Java API Skeleton (f4e075b4f58efd9c9f7f06e943b4e443).
09:10:14.652 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Job recovers via failover strategy: full graph restart
09:10:14.667 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.runtime.jobmaster.JobMaster - Running initialization on master for job Flink Streaming Java API Skeleton (f4e075b4f58efd9c9f7f06e943b4e443).
09:10:14.667 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.runtime.jobmaster.JobMaster - Successfully ran initialization on master in 0 ms.
09:10:14.696 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.runtime.jobmaster.JobMaster - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
09:10:14.705 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.h.n.e.EmbeddedLeaderService - Proposing leadership to contender org.apache.flink.runtime.jobmaster.JobManagerRunner@76defa16 @ akka://flink/user/jobmanager_1
09:10:14.706 [mini-cluster-io-thread-1] INFO  o.a.f.r.jobmaster.JobManagerRunner - JobManager runner for job Flink Streaming Java API Skeleton (f4e075b4f58efd9c9f7f06e943b4e443) was granted leadership with session id a2667b7c-6af8-480c-a538-143b2fc5896c at akka://flink/user/jobmanager_1.
09:10:14.708 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.runtime.jobmaster.JobMaster - Starting execution of job Flink Streaming Java API Skeleton (f4e075b4f58efd9c9f7f06e943b4e443) under job master id a538143b2fc5896ca2667b7c6af8480c.
09:10:14.709 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Job Flink Streaming Java API Skeleton (f4e075b4f58efd9c9f7f06e943b4e443) switched from state CREATED to RUNNING.
09:10:14.713 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (cb8ef90d50a7a9843d2fe6dc35938c5f) switched from CREATED to SCHEDULED.
09:10:14.722 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.j.slotpool.SlotPoolImpl - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{8d8392a3696431a598c8aad14df93fc4}]
09:10:14.727 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (37811312dbe801e3765707b17d179aa3) switched from CREATED to SCHEDULED.
09:10:14.727 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.j.slotpool.SlotPoolImpl - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{081ccb1de44e51779b51e5cb56b2acf8}]
09:10:14.727 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (ba9f2eecbef78920445ebe9100684300) switched from CREATED to SCHEDULED.
09:10:14.727 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.j.slotpool.SlotPoolImpl - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{5ae869e15673ba2fbd7ac3398be5f465}]
09:10:14.727 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (460e96562f37b318b7f526f63a2824d0) switched from CREATED to SCHEDULED.
09:10:14.727 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.j.slotpool.SlotPoolImpl - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{4697e4e11ae25ab13410ab60da1d7431}]
09:10:14.729 [jobmanager-future-thread-1] INFO  o.a.f.r.h.n.e.EmbeddedLeaderService - Received confirmation of leadership for leader akka://flink/user/jobmanager_1 , session=a2667b7c-6af8-480c-a538-143b2fc5896c
09:10:14.730 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.runtime.jobmaster.JobMaster - Connecting to ResourceManager akka://flink/user/resourcemanager(ac49e46180780b710e3ac06a08334eca)
09:10:14.731 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.runtime.jobmaster.JobMaster - Resolved ResourceManager address, beginning registration
09:10:14.731 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.runtime.jobmaster.JobMaster - Registration at ResourceManager attempt 1 (timeout=100ms)
09:10:14.732 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.r.StandaloneResourceManager - Registering job manager a538143b2fc5896ca2667b7c6af8480c@akka://flink/user/jobmanager_1 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:10:14.737 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.r.StandaloneResourceManager - Registered job manager a538143b2fc5896ca2667b7c6af8480c@akka://flink/user/jobmanager_1 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:10:14.738 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.runtime.jobmaster.JobMaster - JobManager successfully registered at ResourceManager, leader id: ac49e46180780b710e3ac06a08334eca.
09:10:14.738 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.j.slotpool.SlotPoolImpl - Requesting new slot [SlotRequestId{8d8392a3696431a598c8aad14df93fc4}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} from resource manager.
09:10:14.739 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.r.StandaloneResourceManager - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} for job f4e075b4f58efd9c9f7f06e943b4e443 with allocation id 15c86a3cd2b6aac7b1358c20073f2cc2.
09:10:14.739 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.j.slotpool.SlotPoolImpl - Requesting new slot [SlotRequestId{081ccb1de44e51779b51e5cb56b2acf8}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} from resource manager.
09:10:14.740 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.j.slotpool.SlotPoolImpl - Requesting new slot [SlotRequestId{5ae869e15673ba2fbd7ac3398be5f465}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} from resource manager.
09:10:14.740 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.j.slotpool.SlotPoolImpl - Requesting new slot [SlotRequestId{4697e4e11ae25ab13410ab60da1d7431}] and profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} from resource manager.
09:10:14.740 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.taskexecutor.TaskExecutor - Receive slot request 15c86a3cd2b6aac7b1358c20073f2cc2 for job f4e075b4f58efd9c9f7f06e943b4e443 from resource manager with leader id ac49e46180780b710e3ac06a08334eca.
09:10:14.741 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.r.StandaloneResourceManager - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} for job f4e075b4f58efd9c9f7f06e943b4e443 with allocation id d5f49f8d8640e147f29048fae4bc145c.
09:10:14.741 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.taskexecutor.TaskExecutor - Allocated slot for 15c86a3cd2b6aac7b1358c20073f2cc2.
09:10:14.741 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.r.StandaloneResourceManager - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} for job f4e075b4f58efd9c9f7f06e943b4e443 with allocation id 701f6e3914aa684d226f0070016c00fb.
09:10:14.741 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.t.JobLeaderService - Add job f4e075b4f58efd9c9f7f06e943b4e443 for job leader monitoring.
09:10:14.741 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.r.StandaloneResourceManager - Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1} for job f4e075b4f58efd9c9f7f06e943b4e443 with allocation id 5ba56a1d16ab9d0aa4929ebe8b348d6a.
09:10:14.742 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.taskexecutor.TaskExecutor - Receive slot request d5f49f8d8640e147f29048fae4bc145c for job f4e075b4f58efd9c9f7f06e943b4e443 from resource manager with leader id ac49e46180780b710e3ac06a08334eca.
09:10:14.742 [mini-cluster-io-thread-4] INFO  o.a.f.r.t.JobLeaderService - Try to register at job manager akka://flink/user/jobmanager_1 with leader id a2667b7c-6af8-480c-a538-143b2fc5896c.
09:10:14.742 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.taskexecutor.TaskExecutor - Allocated slot for d5f49f8d8640e147f29048fae4bc145c.
09:10:14.743 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.t.JobLeaderService - Add job f4e075b4f58efd9c9f7f06e943b4e443 for job leader monitoring.
09:10:14.743 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.taskexecutor.TaskExecutor - Receive slot request 701f6e3914aa684d226f0070016c00fb for job f4e075b4f58efd9c9f7f06e943b4e443 from resource manager with leader id ac49e46180780b710e3ac06a08334eca.
09:10:14.743 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.taskexecutor.TaskExecutor - Allocated slot for 701f6e3914aa684d226f0070016c00fb.
09:10:14.743 [mini-cluster-io-thread-4] INFO  o.a.f.r.t.JobLeaderService - Try to register at job manager akka://flink/user/jobmanager_1 with leader id a2667b7c-6af8-480c-a538-143b2fc5896c.
09:10:14.743 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.t.JobLeaderService - Add job f4e075b4f58efd9c9f7f06e943b4e443 for job leader monitoring.
09:10:14.743 [mini-cluster-io-thread-4] INFO  o.a.f.r.t.JobLeaderService - Try to register at job manager akka://flink/user/jobmanager_1 with leader id a2667b7c-6af8-480c-a538-143b2fc5896c.
09:10:14.743 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.taskexecutor.TaskExecutor - Receive slot request 5ba56a1d16ab9d0aa4929ebe8b348d6a for job f4e075b4f58efd9c9f7f06e943b4e443 from resource manager with leader id ac49e46180780b710e3ac06a08334eca.
09:10:14.743 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.taskexecutor.TaskExecutor - Allocated slot for 5ba56a1d16ab9d0aa4929ebe8b348d6a.
09:10:14.743 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.t.JobLeaderService - Add job f4e075b4f58efd9c9f7f06e943b4e443 for job leader monitoring.
09:10:14.743 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.t.JobLeaderService - Resolved JobManager address, beginning registration
09:10:14.743 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.t.JobLeaderService - Resolved JobManager address, beginning registration
09:10:14.743 [mini-cluster-io-thread-2] INFO  o.a.f.r.t.JobLeaderService - Try to register at job manager akka://flink/user/jobmanager_1 with leader id a2667b7c-6af8-480c-a538-143b2fc5896c.
09:10:14.743 [flink-akka.actor.default-dispatcher-2] INFO  o.a.f.r.t.JobLeaderService - Resolved JobManager address, beginning registration
09:10:14.744 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.t.JobLeaderService - Resolved JobManager address, beginning registration
09:10:14.744 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.t.JobLeaderService - Registration at JobManager attempt 1 (timeout=100ms)
09:10:14.745 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.t.JobLeaderService - Successful registration at job manager akka://flink/user/jobmanager_1 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:10:14.746 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.taskexecutor.TaskExecutor - Establish JobManager connection for job f4e075b4f58efd9c9f7f06e943b4e443.
09:10:14.749 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.taskexecutor.TaskExecutor - Offer reserved slots to the leader of job f4e075b4f58efd9c9f7f06e943b4e443.
09:10:14.753 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (cb8ef90d50a7a9843d2fe6dc35938c5f) switched from SCHEDULED to DEPLOYING.
09:10:14.753 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Deploying Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (attempt #0) to 437bf4a2-7385-435f-98b8-b4a64625e40f @ activate.navicat.com (dataPort=-1)
09:10:14.756 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (37811312dbe801e3765707b17d179aa3) switched from SCHEDULED to DEPLOYING.
09:10:14.756 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Deploying Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (attempt #0) to 437bf4a2-7385-435f-98b8-b4a64625e40f @ activate.navicat.com (dataPort=-1)
09:10:14.756 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (ba9f2eecbef78920445ebe9100684300) switched from SCHEDULED to DEPLOYING.
09:10:14.756 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Deploying Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (attempt #0) to 437bf4a2-7385-435f-98b8-b4a64625e40f @ activate.navicat.com (dataPort=-1)
09:10:14.756 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (460e96562f37b318b7f526f63a2824d0) switched from SCHEDULED to DEPLOYING.
09:10:14.757 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Deploying Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (attempt #0) to 437bf4a2-7385-435f-98b8-b4a64625e40f @ activate.navicat.com (dataPort=-1)
09:10:14.773 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.taskexecutor.TaskExecutor - Received task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4).
09:10:14.774 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.flink.runtime.taskmanager.Task - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (cb8ef90d50a7a9843d2fe6dc35938c5f) switched from CREATED to DEPLOYING.
09:10:14.774 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.flink.runtime.taskmanager.Task - Creating FileSystem stream leak safety net for task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (cb8ef90d50a7a9843d2fe6dc35938c5f) [DEPLOYING]
09:10:14.778 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (cb8ef90d50a7a9843d2fe6dc35938c5f) [DEPLOYING].
09:10:14.778 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.taskexecutor.TaskExecutor - Received task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4).
09:10:14.779 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.flink.runtime.taskmanager.Task - Registering task at network: Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (cb8ef90d50a7a9843d2fe6dc35938c5f) [DEPLOYING].
09:10:14.779 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.flink.runtime.taskmanager.Task - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (37811312dbe801e3765707b17d179aa3) switched from CREATED to DEPLOYING.
09:10:14.780 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.flink.runtime.taskmanager.Task - Creating FileSystem stream leak safety net for task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (37811312dbe801e3765707b17d179aa3) [DEPLOYING]
09:10:14.780 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (37811312dbe801e3765707b17d179aa3) [DEPLOYING].
09:10:14.782 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.taskexecutor.TaskExecutor - Received task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4).
09:10:14.782 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.flink.runtime.taskmanager.Task - Registering task at network: Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (37811312dbe801e3765707b17d179aa3) [DEPLOYING].
09:10:14.783 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.t.slot.TaskSlotTable - Activate slot 701f6e3914aa684d226f0070016c00fb.
09:10:14.783 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.flink.runtime.taskmanager.Task - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (ba9f2eecbef78920445ebe9100684300) switched from CREATED to DEPLOYING.
09:10:14.783 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.t.slot.TaskSlotTable - Activate slot 5ba56a1d16ab9d0aa4929ebe8b348d6a.
09:10:14.783 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.t.slot.TaskSlotTable - Activate slot d5f49f8d8640e147f29048fae4bc145c.
09:10:14.783 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.t.slot.TaskSlotTable - Activate slot 15c86a3cd2b6aac7b1358c20073f2cc2.
09:10:14.783 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.flink.runtime.taskmanager.Task - Creating FileSystem stream leak safety net for task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (ba9f2eecbef78920445ebe9100684300) [DEPLOYING]
09:10:14.784 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (ba9f2eecbef78920445ebe9100684300) [DEPLOYING].
09:10:14.785 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.flink.runtime.taskmanager.Task - Registering task at network: Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (ba9f2eecbef78920445ebe9100684300) [DEPLOYING].
09:10:14.789 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.flink.runtime.taskmanager.Task - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (cb8ef90d50a7a9843d2fe6dc35938c5f) switched from DEPLOYING to RUNNING.
09:10:14.789 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.flink.runtime.taskmanager.Task - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (ba9f2eecbef78920445ebe9100684300) switched from DEPLOYING to RUNNING.
09:10:14.789 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.flink.runtime.taskmanager.Task - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (37811312dbe801e3765707b17d179aa3) switched from DEPLOYING to RUNNING.
09:10:14.789 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4) (ba9f2eecbef78920445ebe9100684300) switched from DEPLOYING to RUNNING.
09:10:14.790 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4) (cb8ef90d50a7a9843d2fe6dc35938c5f) switched from DEPLOYING to RUNNING.
09:10:14.790 [flink-akka.actor.default-dispatcher-3] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4) (37811312dbe801e3765707b17d179aa3) switched from DEPLOYING to RUNNING.
09:10:14.791 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.runtime.tasks.StreamTask - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
09:10:14.791 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.runtime.tasks.StreamTask - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
09:10:14.791 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.runtime.tasks.StreamTask - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
09:10:14.789 [flink-akka.actor.default-dispatcher-5] INFO  o.a.f.r.taskexecutor.TaskExecutor - Received task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4).
09:10:14.792 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.flink.runtime.taskmanager.Task - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (460e96562f37b318b7f526f63a2824d0) switched from CREATED to DEPLOYING.
09:10:14.792 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.flink.runtime.taskmanager.Task - Creating FileSystem stream leak safety net for task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (460e96562f37b318b7f526f63a2824d0) [DEPLOYING]
09:10:14.792 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (460e96562f37b318b7f526f63a2824d0) [DEPLOYING].
09:10:14.794 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.flink.runtime.taskmanager.Task - Registering task at network: Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (460e96562f37b318b7f526f63a2824d0) [DEPLOYING].
09:10:14.804 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.flink.runtime.taskmanager.Task - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (460e96562f37b318b7f526f63a2824d0) switched from DEPLOYING to RUNNING.
09:10:14.805 [flink-akka.actor.default-dispatcher-4] INFO  o.a.f.r.e.ExecutionGraph - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4) (460e96562f37b318b7f526f63a2824d0) switched from DEPLOYING to RUNNING.
09:10:14.805 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.runtime.tasks.StreamTask - No state backend has been configured, using default (Memory / JobManager) MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null', asynchronous: TRUE, maxStateSize: 5242880)
09:10:14.844 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - no state to restore
09:10:14.844 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - no state to restore
09:10:14.844 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - no state to restore
09:10:14.844 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - no state to restore
09:10:14.860 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:10:14.860 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:10:14.861 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:10:14.861 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

09:10:14.933 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:10:14.934 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:10:14.935 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.c.k.FlinkKafkaProducer011 - Starting FlinkKafkaProducer (1/4) to produce into default topic test1
09:10:14.944 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.a.java.typeutils.TypeExtractor - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
09:10:14.944 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.a.java.typeutils.TypeExtractor - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
09:10:14.948 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:10:14.948 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:10:14.948 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.c.k.FlinkKafkaProducer011 - Starting FlinkKafkaProducer (3/4) to produce into default topic test1
09:10:14.949 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.a.java.typeutils.TypeExtractor - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
09:10:14.949 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.a.java.typeutils.TypeExtractor - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
09:10:14.954 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:10:14.954 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:10:14.954 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.c.k.FlinkKafkaProducer011 - Starting FlinkKafkaProducer (4/4) to produce into default topic test1
09:10:14.955 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.a.java.typeutils.TypeExtractor - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
09:10:14.955 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.a.java.typeutils.TypeExtractor - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
09:10:14.956 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 3 has no restore state.
09:10:14.957 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 has no restore state.
09:10:14.959 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 0 has no restore state.
09:10:14.963 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:10:14.963 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:10:14.963 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:10:14.970 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:10:14.970 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:10:14.970 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.c.k.FlinkKafkaProducer011 - Starting FlinkKafkaProducer (2/4) to produce into default topic test1
09:10:14.985 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.a.java.typeutils.TypeExtractor - class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition does not contain a setter for field topic
09:10:14.989 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.a.java.typeutils.TypeExtractor - Class class org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance.
09:10:14.989 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 1 has no restore state.
09:10:14.990 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:10:15.023 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.k.c.consumer.ConsumerConfig - The configuration 'zookeeper.connect' was supplied but isn't a known config.
09:10:15.023 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] WARN  o.a.k.c.consumer.ConsumerConfig - The configuration 'zookeeper.connect' was supplied but isn't a known config.
09:10:15.023 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:10:15.024 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:10:15.024 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] WARN  o.a.k.c.consumer.ConsumerConfig - The configuration 'zookeeper.connect' was supplied but isn't a known config.
09:10:15.025 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:10:15.025 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:10:15.025 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:10:15.025 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:10:15.040 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] WARN  o.a.k.c.consumer.ConsumerConfig - The configuration 'zookeeper.connect' was supplied but isn't a known config.
09:10:15.040 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:10:15.040 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:10:15.113 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 0 initially has no partitions to read from.
09:10:15.113 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 3 initially has no partitions to read from.
09:10:15.113 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 1 initially has no partitions to read from.
09:10:15.113 [Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 will start reading the following 1 partitions from the latest offsets: [KafkaTopicPartition{topic='test', partition=0}]
09:10:15.118 [Legacy Source Thread - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 1 creating fetcher with offsets {}.
09:10:15.118 [Legacy Source Thread - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 0 creating fetcher with offsets {}.
09:10:15.118 [Legacy Source Thread - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 3 creating fetcher with offsets {}.
09:10:15.118 [Legacy Source Thread - Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 creating fetcher with offsets {KafkaTopicPartition{topic='test', partition=0}=-915623761774}.
09:10:15.123 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:10:15.123 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:10:15.124 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:10:15.149 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.k.c.consumer.ConsumerConfig - The configuration 'zookeeper.connect' was supplied but isn't a known config.
09:10:15.149 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:10:15.149 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:10:15.151 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] WARN  o.a.k.c.consumer.ConsumerConfig - The configuration 'zookeeper.connect' was supplied but isn't a known config.
09:10:15.151 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:10:15.151 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:10:15.152 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

09:10:15.179 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] WARN  o.a.k.c.consumer.ConsumerConfig - The configuration 'zookeeper.connect' was supplied but isn't a known config.
09:10:15.180 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:10:15.180 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:10:15.180 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] WARN  o.a.k.c.consumer.ConsumerConfig - The configuration 'zookeeper.connect' was supplied but isn't a known config.
09:10:15.180 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:10:15.180 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:10:15.682 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 1 @ 1595898615660 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:10:15.909 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 1 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 249 ms).
09:10:15.910 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 1 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898614977} from checkpoint 1
09:10:15.910 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 1 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898614943} from checkpoint 1
09:10:15.910 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 1 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898614948} from checkpoint 1
09:10:15.910 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 1 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898614955} from checkpoint 1
09:10:16.204 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.k.c.c.i.AbstractCoordinator - Discovered coordinator sx-9633.dtdream.com:9092 (id: 2147483647 rack: null) for group test-consumer-group.
09:10:20.660 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 2 @ 1595898620660 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:10:20.671 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 2 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 9 ms).
09:10:20.672 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 2 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898615701} from checkpoint 2
09:10:20.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 2 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898615701} from checkpoint 2
09:10:20.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 2 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898615702} from checkpoint 2
09:10:20.674 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 2 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898615702} from checkpoint 2
09:10:25.660 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 3 @ 1595898625660 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:10:25.680 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 3 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 20 ms).
09:10:25.681 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 3 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898620662} from checkpoint 3
09:10:25.682 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 3 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898620662} from checkpoint 3
09:10:25.681 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 3 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898620662} from checkpoint 3
09:10:25.682 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 3 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898620663} from checkpoint 3
09:10:30.678 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 4 @ 1595898630678 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:10:30.730 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 4 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 52 ms).
09:10:30.731 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 4 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898625663} from checkpoint 4
09:10:30.732 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 4 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898625667} from checkpoint 4
09:10:30.732 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 4 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898625666} from checkpoint 4
09:10:30.732 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 4 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898625666} from checkpoint 4
09:10:32.887 [main] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

09:10:33.464 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:10:33.464 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:10:33.617 [main] INFO  o.a.k.clients.producer.KafkaProducer - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
09:10:35.661 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 5 @ 1595898635661 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:10:35.686 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 5 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 25 ms).
09:10:35.687 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 5 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898630720} from checkpoint 5
09:10:35.689 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 5 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898630728} from checkpoint 5
09:10:35.688 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 5 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898630725} from checkpoint 5
09:10:35.687 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 5 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898630723} from checkpoint 5
09:10:40.660 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 6 @ 1595898640660 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:10:40.668 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 6 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 8 ms).
09:10:40.668 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 6 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898635666} from checkpoint 6
09:10:40.669 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 6 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898635666} from checkpoint 6
09:10:40.669 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 6 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898635667} from checkpoint 6
09:10:40.670 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 6 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898635682} from checkpoint 6
09:10:45.661 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 7 @ 1595898645661 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:10:45.666 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 7 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 5 ms).
09:10:45.666 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 7 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898640661} from checkpoint 7
09:10:45.668 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 7 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898640661} from checkpoint 7
09:10:45.668 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 7 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898640661} from checkpoint 7
09:10:45.668 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 7 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898640661} from checkpoint 7
09:10:50.659 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 8 @ 1595898650659 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:10:50.677 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 8 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 17 ms).
09:10:50.678 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 8 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898645662} from checkpoint 8
09:10:50.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 8 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898645662} from checkpoint 8
09:10:50.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 8 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898645662} from checkpoint 8
09:10:50.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 8 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898645662} from checkpoint 8
09:10:55.659 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 9 @ 1595898655659 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:10:55.662 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 9 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 3 ms).
09:10:55.662 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 9 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898650662} from checkpoint 9
09:10:55.663 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 9 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898650664} from checkpoint 9
09:10:55.663 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 9 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898650663} from checkpoint 9
09:10:55.664 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 9 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898650666} from checkpoint 9
09:11:00.660 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 10 @ 1595898660660 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:11:00.670 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 10 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 10 ms).
09:11:00.677 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 10 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898655659} from checkpoint 10
09:11:00.678 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 10 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898655660} from checkpoint 10
09:11:00.678 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 10 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898655660} from checkpoint 10
09:11:00.678 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 10 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898655660} from checkpoint 10
09:11:04.768 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

09:11:05.292 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:11:05.292 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:11:05.424 [main] INFO  o.a.k.c.c.i.AbstractCoordinator - Discovered coordinator sx-9633.dtdream.com:9092 (id: 2147483647 rack: null) for group test-consumer-group.
09:11:05.430 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - Revoking previously assigned partitions [] for group test-consumer-group
09:11:05.431 [main] INFO  o.a.k.c.c.i.AbstractCoordinator - (Re-)joining group test-consumer-group
09:11:05.451 [main] INFO  o.a.k.c.c.i.AbstractCoordinator - Successfully joined group test-consumer-group with generation 3
09:11:05.452 [main] INFO  o.a.k.c.c.i.ConsumerCoordinator - Setting newly assigned partitions [test1-0] for group test-consumer-group
09:11:05.661 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 11 @ 1595898665661 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:11:05.665 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 11 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 4 ms).
09:11:05.666 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 11 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898660663} from checkpoint 11
09:11:05.666 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 11 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898660663} from checkpoint 11
09:11:05.666 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 11 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898660663} from checkpoint 11
09:11:05.665 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 11 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898660661} from checkpoint 11
09:11:05.670 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:05.671 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:10.661 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 12 @ 1595898670661 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:11:10.668 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 12 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 7 ms).
09:11:10.670 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 12 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898665662} from checkpoint 12
09:11:10.670 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 12 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898665662} from checkpoint 12
09:11:10.670 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 12 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898665662} from checkpoint 12
09:11:10.669 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 12 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898665662} from checkpoint 12
09:11:10.672 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:10.672 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:15.665 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 13 @ 1595898675665 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:11:15.675 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 13 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 10 ms).
09:11:15.677 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 13 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898670662} from checkpoint 13
09:11:15.677 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 13 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898670662} from checkpoint 13
09:11:15.676 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 13 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898670661} from checkpoint 13
09:11:15.678 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:15.679 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:15.676 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 13 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898670661} from checkpoint 13
09:11:19.534 [main] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

09:11:20.067 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:11:20.067 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:11:20.257 [main] INFO  o.a.k.clients.producer.KafkaProducer - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
09:11:20.661 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 14 @ 1595898680661 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:11:20.664 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 14 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 3 ms).
09:11:20.665 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 14 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898675672} from checkpoint 14
09:11:20.665 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 14 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898675672} from checkpoint 14
09:11:20.665 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 14 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898675672} from checkpoint 14
09:11:20.665 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 14 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898675672} from checkpoint 14
09:11:20.666 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:20.667 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:25.661 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 15 @ 1595898685661 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:11:25.678 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 15 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 16 ms).
09:11:25.678 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 15 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898680661} from checkpoint 15
09:11:25.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 15 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898680662} from checkpoint 15
09:11:25.678 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 15 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898680662} from checkpoint 15
09:11:25.678 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 15 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898680662} from checkpoint 15
09:11:25.680 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:25.681 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:30.659 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 16 @ 1595898690659 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:11:30.689 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 16 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 30 ms).
09:11:30.694 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 16 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898685664} from checkpoint 16
09:11:30.696 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 16 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898685665} from checkpoint 16
09:11:30.696 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 16 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898685665} from checkpoint 16
09:11:30.695 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 16 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898685664} from checkpoint 16
09:11:30.702 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:30.703 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:35.660 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 17 @ 1595898695660 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:11:35.673 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 17 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 13 ms).
09:11:35.675 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 17 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898690663} from checkpoint 17
09:11:35.675 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 17 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898690664} from checkpoint 17
09:11:35.675 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 17 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898690664} from checkpoint 17
09:11:35.675 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 17 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898690664} from checkpoint 17
09:11:35.680 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:35.682 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:40.661 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 18 @ 1595898700661 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:11:40.673 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 18 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 12 ms).
09:11:40.677 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 18 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898695663} from checkpoint 18
09:11:40.677 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 18 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898695663} from checkpoint 18
09:11:40.675 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 18 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898695663} from checkpoint 18
09:11:40.674 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 18 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898695662} from checkpoint 18
09:11:40.680 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:40.682 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:45.665 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 19 @ 1595898705665 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:11:45.676 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 19 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 10 ms).
09:11:45.676 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 19 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898700663} from checkpoint 19
09:11:45.677 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 19 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898700664} from checkpoint 19
09:11:45.677 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 19 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898700665} from checkpoint 19
09:11:45.678 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 19 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898700666} from checkpoint 19
09:11:45.681 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:45.683 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:50.661 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 20 @ 1595898710661 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:11:50.673 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 20 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 12 ms).
09:11:50.674 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 20 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898705667} from checkpoint 20
09:11:50.675 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 20 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898705671} from checkpoint 20
09:11:50.675 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 20 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898705669} from checkpoint 20
09:11:50.674 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 20 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898705668} from checkpoint 20
09:11:50.678 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:50.679 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:55.659 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 21 @ 1595898715659 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:11:55.670 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 21 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 10 ms).
09:11:55.670 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 21 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898710663} from checkpoint 21
09:11:55.671 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 21 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898710664} from checkpoint 21
09:11:55.671 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 21 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898710664} from checkpoint 21
09:11:55.671 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 21 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898710665} from checkpoint 21
09:11:55.675 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:11:55.677 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:00.661 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 22 @ 1595898720661 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:12:00.680 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 22 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 18 ms).
09:12:00.683 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 22 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898715663} from checkpoint 22
09:12:00.682 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 22 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898715662} from checkpoint 22
09:12:00.682 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 22 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898715662} from checkpoint 22
09:12:00.682 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 22 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898715662} from checkpoint 22
09:12:00.689 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:00.690 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:05.660 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 23 @ 1595898725660 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:12:05.670 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 23 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 10 ms).
09:12:05.671 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 23 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898720663} from checkpoint 23
09:12:05.671 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 23 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898720665} from checkpoint 23
09:12:05.672 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 23 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898720665} from checkpoint 23
09:12:05.671 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 23 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898720664} from checkpoint 23
09:12:05.676 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:05.678 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:10.660 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 24 @ 1595898730660 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:12:10.672 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 24 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 11 ms).
09:12:10.672 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 24 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898725662} from checkpoint 24
09:12:10.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 24 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898725663} from checkpoint 24
09:12:10.675 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 24 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898725663} from checkpoint 24
09:12:10.674 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 24 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898725663} from checkpoint 24
09:12:10.679 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:10.680 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:15.659 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 25 @ 1595898735659 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:12:15.670 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 25 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 10 ms).
09:12:15.670 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 25 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898730662} from checkpoint 25
09:12:15.671 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 25 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898730664} from checkpoint 25
09:12:15.671 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 25 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898730665} from checkpoint 25
09:12:15.671 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 25 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898730663} from checkpoint 25
09:12:15.676 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:15.678 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:20.660 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 26 @ 1595898740660 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:12:20.671 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 26 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 11 ms).
09:12:20.672 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 26 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898735662} from checkpoint 26
09:12:20.674 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 26 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898735663} from checkpoint 26
09:12:20.674 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 26 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898735663} from checkpoint 26
09:12:20.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 26 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898735662} from checkpoint 26
09:12:20.679 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:20.680 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:25.664 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 27 @ 1595898745664 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:12:25.674 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 27 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 10 ms).
09:12:25.675 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 27 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898740662} from checkpoint 27
09:12:25.676 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 27 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898740663} from checkpoint 27
09:12:25.676 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 27 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898740663} from checkpoint 27
09:12:25.675 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 27 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898740663} from checkpoint 27
09:12:25.680 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:25.682 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:30.661 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 28 @ 1595898750661 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:12:30.671 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 28 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 10 ms).
09:12:30.672 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 28 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898745666} from checkpoint 28
09:12:30.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 28 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898745667} from checkpoint 28
09:12:30.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 28 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898745667} from checkpoint 28
09:12:30.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 28 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898745667} from checkpoint 28
09:12:30.677 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:30.678 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:35.663 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 29 @ 1595898755663 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:12:35.665 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 29 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 2 ms).
09:12:35.665 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 29 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898750663} from checkpoint 29
09:12:35.665 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 29 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898750664} from checkpoint 29
09:12:35.665 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 29 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898750664} from checkpoint 29
09:12:35.666 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 29 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898750664} from checkpoint 29
09:12:35.667 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:35.668 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:40.663 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 30 @ 1595898760663 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:12:40.675 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 30 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 11 ms).
09:12:40.675 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 30 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898755663} from checkpoint 30
09:12:40.677 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 30 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898755664} from checkpoint 30
09:12:40.677 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 30 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898755663} from checkpoint 30
09:12:40.676 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 30 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898755663} from checkpoint 30
09:12:40.681 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:40.683 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:45.663 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 31 @ 1595898765663 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:12:45.668 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 31 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 5 ms).
09:12:45.669 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 31 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898760665} from checkpoint 31
09:12:45.670 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 31 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898760667} from checkpoint 31
09:12:45.670 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 31 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898760668} from checkpoint 31
09:12:45.670 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 31 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898760667} from checkpoint 31
09:12:45.672 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:45.673 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:50.663 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 32 @ 1595898770663 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:12:50.674 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 32 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 11 ms).
09:12:50.676 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 32 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898765664} from checkpoint 32
09:12:50.677 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 32 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898765664} from checkpoint 32
09:12:50.678 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 32 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898765665} from checkpoint 32
09:12:50.677 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 32 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898765664} from checkpoint 32
09:12:50.682 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:50.684 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:55.665 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 33 @ 1595898775665 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:12:55.673 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 33 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 8 ms).
09:12:55.674 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 33 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898770665} from checkpoint 33
09:12:55.674 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 33 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898770666} from checkpoint 33
09:12:55.674 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 33 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898770666} from checkpoint 33
09:12:55.675 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 33 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898770667} from checkpoint 33
09:12:55.680 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:12:55.681 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:00.666 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 34 @ 1595898780666 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:13:00.676 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 34 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 10 ms).
09:13:00.677 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 34 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898775667} from checkpoint 34
09:13:00.680 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 34 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898775667} from checkpoint 34
09:13:00.680 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 34 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898775667} from checkpoint 34
09:13:00.680 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 34 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898775667} from checkpoint 34
09:13:00.684 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:00.685 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:05.668 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 35 @ 1595898785668 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:13:05.672 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 35 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 3 ms).
09:13:05.672 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 35 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898780668} from checkpoint 35
09:13:05.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 35 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898780669} from checkpoint 35
09:13:05.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 35 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898780670} from checkpoint 35
09:13:05.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 35 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898780669} from checkpoint 35
09:13:05.674 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:05.674 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:10.667 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 36 @ 1595898790667 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:13:10.670 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 36 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 3 ms).
09:13:10.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 36 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898785670} from checkpoint 36
09:13:10.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 36 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898785670} from checkpoint 36
09:13:10.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 36 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898785670} from checkpoint 36
09:13:10.672 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 36 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898785669} from checkpoint 36
09:13:10.674 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:10.675 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:12.555 [main] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

09:13:13.051 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:13:13.051 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:13:13.196 [main] INFO  o.a.k.clients.producer.KafkaProducer - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
09:13:15.666 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 37 @ 1595898795666 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:13:15.668 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 37 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 2 ms).
09:13:15.669 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 37 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898790668} from checkpoint 37
09:13:15.669 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 37 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898790668} from checkpoint 37
09:13:15.669 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 37 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898790668} from checkpoint 37
09:13:15.669 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 37 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898790668} from checkpoint 37
09:13:15.670 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:15.670 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:20.667 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 38 @ 1595898800667 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:13:20.675 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 38 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 8 ms).
09:13:20.675 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 38 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898795667} from checkpoint 38
09:13:20.676 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 38 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898795667} from checkpoint 38
09:13:20.676 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 38 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898795667} from checkpoint 38
09:13:20.675 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 38 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898795667} from checkpoint 38
09:13:20.678 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:20.679 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:24.803 [main] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

09:13:25.349 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:13:25.351 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:13:25.487 [main] INFO  o.a.k.clients.producer.KafkaProducer - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
09:13:25.668 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 39 @ 1595898805668 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:13:25.670 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 39 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 2 ms).
09:13:25.670 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 39 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898800669} from checkpoint 39
09:13:25.671 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 39 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898800669} from checkpoint 39
09:13:25.671 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 39 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898800668} from checkpoint 39
09:13:25.671 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 39 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898800669} from checkpoint 39
09:13:25.672 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:25.672 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:30.669 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 40 @ 1595898810669 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:13:30.672 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 40 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 3 ms).
09:13:30.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 40 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898805668} from checkpoint 40
09:13:30.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 40 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898805668} from checkpoint 40
09:13:30.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 40 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898805668} from checkpoint 40
09:13:30.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 40 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898805668} from checkpoint 40
09:13:30.675 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:30.675 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:35.670 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 41 @ 1595898815670 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:13:35.673 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 41 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 3 ms).
09:13:35.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 41 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898810670} from checkpoint 41
09:13:35.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 41 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898810670} from checkpoint 41
09:13:35.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 41 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898810670} from checkpoint 41
09:13:35.673 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 41 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898810670} from checkpoint 41
09:13:35.674 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:35.675 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:40.669 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 42 @ 1595898820669 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:13:40.684 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 42 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 15 ms).
09:13:40.685 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 42 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898815671} from checkpoint 42
09:13:40.686 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 42 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898815671} from checkpoint 42
09:13:40.685 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 42 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898815671} from checkpoint 42
09:13:40.685 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 42 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898815671} from checkpoint 42
09:13:40.689 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:40.690 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:45.671 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 43 @ 1595898825671 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:13:45.674 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 43 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 3 ms).
09:13:45.674 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 43 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898820671} from checkpoint 43
09:13:45.675 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 43 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898820672} from checkpoint 43
09:13:45.674 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 43 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898820672} from checkpoint 43
09:13:45.674 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 43 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898820672} from checkpoint 43
09:13:45.676 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:45.676 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:50.672 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 44 @ 1595898830672 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:13:50.691 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 44 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 19 ms).
09:13:50.692 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 44 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898825671} from checkpoint 44
09:13:50.693 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 44 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898825672} from checkpoint 44
09:13:50.693 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 44 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898825671} from checkpoint 44
09:13:50.692 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 44 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898825671} from checkpoint 44
09:13:50.698 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:50.699 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:55.679 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 45 @ 1595898835679 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:13:55.686 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 45 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 7 ms).
09:13:55.687 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 45 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898830677} from checkpoint 45
09:13:55.688 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 45 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898830676} from checkpoint 45
09:13:55.687 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 45 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898830676} from checkpoint 45
09:13:55.688 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 45 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898830677} from checkpoint 45
09:13:55.690 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:13:55.691 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:00.672 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 46 @ 1595898840672 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:14:00.678 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 46 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:14:00.678 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 46 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898835680} from checkpoint 46
09:14:00.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 46 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898835681} from checkpoint 46
09:14:00.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 46 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898835680} from checkpoint 46
09:14:00.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 46 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898835680} from checkpoint 46
09:14:00.682 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:00.683 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:05.674 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 47 @ 1595898845674 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:14:05.686 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 47 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 12 ms).
09:14:05.686 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 47 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898840674} from checkpoint 47
09:14:05.688 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 47 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898840674} from checkpoint 47
09:14:05.687 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 47 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898840674} from checkpoint 47
09:14:05.687 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 47 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898840674} from checkpoint 47
09:14:05.691 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:05.693 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:10.675 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 48 @ 1595898850675 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:14:10.679 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 48 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 3 ms).
09:14:10.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 48 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898845676} from checkpoint 48
09:14:10.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 48 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898845679} from checkpoint 48
09:14:10.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 48 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898845677} from checkpoint 48
09:14:10.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 48 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898845677} from checkpoint 48
09:14:10.680 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:10.681 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:15.675 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 49 @ 1595898855675 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:14:15.683 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 49 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 8 ms).
09:14:15.684 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 49 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898850675} from checkpoint 49
09:14:15.686 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 49 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898850675} from checkpoint 49
09:14:15.686 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 49 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898850676} from checkpoint 49
09:14:15.685 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 49 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898850675} from checkpoint 49
09:14:15.689 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:15.690 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:20.676 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 50 @ 1595898860676 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:14:20.679 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 50 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 3 ms).
09:14:20.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 50 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898855677} from checkpoint 50
09:14:20.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 50 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898855678} from checkpoint 50
09:14:20.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 50 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898855677} from checkpoint 50
09:14:20.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 50 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898855677} from checkpoint 50
09:14:20.680 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:20.681 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:25.676 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 51 @ 1595898865676 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:14:25.678 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 51 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 2 ms).
09:14:25.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 51 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898860677} from checkpoint 51
09:14:25.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 51 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898860677} from checkpoint 51
09:14:25.679 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 51 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898860677} from checkpoint 51
09:14:25.678 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 51 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898860677} from checkpoint 51
09:14:25.680 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:25.681 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:30.677 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 52 @ 1595898870677 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:14:30.687 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 52 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 10 ms).
09:14:30.687 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 52 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898865676} from checkpoint 52
09:14:30.688 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 52 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898865677} from checkpoint 52
09:14:30.688 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 52 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898865676} from checkpoint 52
09:14:30.688 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 52 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898865676} from checkpoint 52
09:14:30.692 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:30.693 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:35.677 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 53 @ 1595898875677 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:14:35.687 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 53 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 10 ms).
09:14:35.688 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 53 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898870679} from checkpoint 53
09:14:35.688 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 53 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898870680} from checkpoint 53
09:14:35.688 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 53 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898870680} from checkpoint 53
09:14:35.688 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 53 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898870679} from checkpoint 53
09:14:35.691 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:35.692 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:40.678 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 54 @ 1595898880678 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:14:40.680 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 54 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 2 ms).
09:14:40.681 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 54 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898875679} from checkpoint 54
09:14:40.681 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 54 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898875679} from checkpoint 54
09:14:40.681 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 54 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898875680} from checkpoint 54
09:14:40.681 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 54 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898875680} from checkpoint 54
09:14:40.682 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:40.682 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:45.678 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 55 @ 1595898885678 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:14:45.680 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 55 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 2 ms).
09:14:45.680 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 55 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898880678} from checkpoint 55
09:14:45.680 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 55 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898880679} from checkpoint 55
09:14:45.680 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 55 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898880679} from checkpoint 55
09:14:45.680 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 55 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898880678} from checkpoint 55
09:14:45.681 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:45.682 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:50.678 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 56 @ 1595898890678 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:14:50.689 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 56 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 11 ms).
09:14:50.692 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 56 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898885678} from checkpoint 56
09:14:50.693 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 56 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898885678} from checkpoint 56
09:14:50.693 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 56 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898885678} from checkpoint 56
09:14:50.693 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 56 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898885678} from checkpoint 56
09:14:50.697 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:50.698 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:55.679 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 57 @ 1595898895679 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:14:55.686 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 57 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 7 ms).
09:14:55.687 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 57 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898890680} from checkpoint 57
09:14:55.687 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 57 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898890680} from checkpoint 57
09:14:55.690 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 57 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898890680} from checkpoint 57
09:14:55.690 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 57 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898890681} from checkpoint 57
09:14:55.694 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:14:55.695 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:00.680 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 58 @ 1595898900680 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:15:00.689 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 58 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 9 ms).
09:15:00.690 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 58 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898895681} from checkpoint 58
09:15:00.691 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 58 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898895682} from checkpoint 58
09:15:00.690 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 58 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898895682} from checkpoint 58
09:15:00.690 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 58 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898895681} from checkpoint 58
09:15:00.694 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:00.696 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:05.681 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 59 @ 1595898905681 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:15:05.690 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 59 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 9 ms).
09:15:05.691 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 59 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898900682} from checkpoint 59
09:15:05.691 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 59 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898900683} from checkpoint 59
09:15:05.691 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 59 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898900682} from checkpoint 59
09:15:05.691 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 59 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898900683} from checkpoint 59
09:15:05.695 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:05.697 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:10.682 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 60 @ 1595898910682 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:15:10.692 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 60 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 9 ms).
09:15:10.692 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 60 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898905683} from checkpoint 60
09:15:10.693 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 60 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898905684} from checkpoint 60
09:15:10.693 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 60 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898905684} from checkpoint 60
09:15:10.692 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 60 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898905683} from checkpoint 60
09:15:10.697 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:10.699 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:15.683 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 61 @ 1595898915683 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:15:15.685 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 61 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 2 ms).
09:15:15.685 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 61 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898910684} from checkpoint 61
09:15:15.686 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 61 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898910684} from checkpoint 61
09:15:15.686 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 61 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898910685} from checkpoint 61
09:15:15.686 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 61 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898910685} from checkpoint 61
09:15:15.687 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:15.687 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:20.682 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 62 @ 1595898920682 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:15:20.693 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 62 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 11 ms).
09:15:20.694 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 62 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898915683} from checkpoint 62
09:15:20.695 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 62 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898915684} from checkpoint 62
09:15:20.695 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 62 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898915683} from checkpoint 62
09:15:20.694 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 62 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898915683} from checkpoint 62
09:15:20.699 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:20.700 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:25.683 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 63 @ 1595898925683 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:15:25.697 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 63 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 13 ms).
09:15:25.697 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 63 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898920684} from checkpoint 63
09:15:25.698 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 63 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898920685} from checkpoint 63
09:15:25.699 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 63 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898920686} from checkpoint 63
09:15:25.698 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 63 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898920685} from checkpoint 63
09:15:25.702 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:25.703 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:30.688 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 64 @ 1595898930687 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:15:30.719 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 64 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 32 ms).
09:15:30.720 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 64 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898925687} from checkpoint 64
09:15:30.720 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 64 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898925686} from checkpoint 64
09:15:30.720 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 64 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898925686} from checkpoint 64
09:15:30.719 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 64 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898925685} from checkpoint 64
09:15:30.722 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:30.723 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:35.684 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 65 @ 1595898935684 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:15:35.695 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 65 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 11 ms).
09:15:35.697 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 65 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898930701} from checkpoint 65
09:15:35.698 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 65 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898930713} from checkpoint 65
09:15:35.698 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 65 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898930712} from checkpoint 65
09:15:35.698 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 65 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898930702} from checkpoint 65
09:15:35.701 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:35.702 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:40.686 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 66 @ 1595898940686 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:15:40.697 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 66 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 11 ms).
09:15:40.698 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 66 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898935685} from checkpoint 66
09:15:40.699 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 66 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898935687} from checkpoint 66
09:15:40.698 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 66 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898935685} from checkpoint 66
09:15:40.698 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 66 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898935685} from checkpoint 66
09:15:40.704 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:40.704 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:45.685 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 67 @ 1595898945685 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:15:45.689 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 67 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 4 ms).
09:15:45.690 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 67 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898940689} from checkpoint 67
09:15:45.690 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 67 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898940690} from checkpoint 67
09:15:45.690 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 67 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898940688} from checkpoint 67
09:15:45.689 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 67 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898940689} from checkpoint 67
09:15:45.691 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:45.692 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:48.170 [main] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

09:15:48.692 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:15:48.704 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:15:48.865 [main] INFO  o.a.k.clients.producer.KafkaProducer - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
09:15:50.686 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 68 @ 1595898950686 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:15:50.694 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 68 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:15:50.695 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 68 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898945686} from checkpoint 68
09:15:50.695 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 68 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898945687} from checkpoint 68
09:15:50.695 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 68 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898945686} from checkpoint 68
09:15:50.695 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 68 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898945686} from checkpoint 68
09:15:50.698 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:50.699 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:55.687 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 69 @ 1595898955687 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:15:55.695 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 69 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 7 ms).
09:15:55.696 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 69 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898950688} from checkpoint 69
09:15:55.695 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 69 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898950688} from checkpoint 69
09:15:55.695 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 69 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898950688} from checkpoint 69
09:15:55.695 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 69 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898950687} from checkpoint 69
09:15:55.698 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:15:55.700 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:00.689 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 70 @ 1595898960689 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:16:00.700 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 70 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 10 ms).
09:16:00.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 70 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898955688} from checkpoint 70
09:16:00.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 70 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898955688} from checkpoint 70
09:16:00.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 70 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898955688} from checkpoint 70
09:16:00.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 70 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898955688} from checkpoint 70
09:16:00.706 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:00.708 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:05.688 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 71 @ 1595898965688 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:16:05.713 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 71 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 25 ms).
09:16:05.714 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 71 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898960693} from checkpoint 71
09:16:05.714 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 71 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898960692} from checkpoint 71
09:16:05.714 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 71 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898960691} from checkpoint 71
09:16:05.717 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:05.718 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:05.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 71 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898960691} from checkpoint 71
09:16:10.689 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 72 @ 1595898970689 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:16:10.697 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 72 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 8 ms).
09:16:10.698 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 72 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898965690} from checkpoint 72
09:16:10.699 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 72 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898965712} from checkpoint 72
09:16:10.699 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 72 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898965692} from checkpoint 72
09:16:10.699 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 72 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898965691} from checkpoint 72
09:16:10.703 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:10.705 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:15.689 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 73 @ 1595898975689 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:16:15.697 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 73 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 8 ms).
09:16:15.698 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 73 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898970691} from checkpoint 73
09:16:15.698 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 73 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898970692} from checkpoint 73
09:16:15.698 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 73 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898970692} from checkpoint 73
09:16:15.698 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 73 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898970692} from checkpoint 73
09:16:15.702 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:15.704 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:20.691 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 74 @ 1595898980691 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:16:20.693 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 74 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 2 ms).
09:16:20.694 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 74 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898975691} from checkpoint 74
09:16:20.694 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 74 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898975691} from checkpoint 74
09:16:20.694 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 74 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898975691} from checkpoint 74
09:16:20.694 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 74 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898975691} from checkpoint 74
09:16:20.696 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:20.696 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:25.691 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 75 @ 1595898985691 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:16:25.696 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 75 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 5 ms).
09:16:25.697 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 75 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898980692} from checkpoint 75
09:16:25.696 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 75 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898980692} from checkpoint 75
09:16:25.697 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 75 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898980692} from checkpoint 75
09:16:25.697 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 75 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898980692} from checkpoint 75
09:16:25.700 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:25.701 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:30.691 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 76 @ 1595898990691 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:16:30.697 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 76 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:16:30.697 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 76 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898985693} from checkpoint 76
09:16:30.697 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 76 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898985692} from checkpoint 76
09:16:30.697 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 76 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898985692} from checkpoint 76
09:16:30.697 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 76 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898985693} from checkpoint 76
09:16:30.699 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:30.701 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:35.691 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 77 @ 1595898995691 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:16:35.693 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 77 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 2 ms).
09:16:35.695 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 77 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898990692} from checkpoint 77
09:16:35.695 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 77 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898990694} from checkpoint 77
09:16:35.695 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 77 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898990694} from checkpoint 77
09:16:35.695 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 77 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898990693} from checkpoint 77
09:16:35.697 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:35.698 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:40.693 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 78 @ 1595899000693 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:16:40.697 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 78 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 4 ms).
09:16:40.697 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 78 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898995691} from checkpoint 78
09:16:40.697 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 78 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898995691} from checkpoint 78
09:16:40.697 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 78 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898995692} from checkpoint 78
09:16:40.697 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 78 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595898995692} from checkpoint 78
09:16:40.698 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:40.699 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:45.694 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 79 @ 1595899005694 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:16:45.707 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 79 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 13 ms).
09:16:45.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 79 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899000694} from checkpoint 79
09:16:45.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 79 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899000694} from checkpoint 79
09:16:45.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 79 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899000693} from checkpoint 79
09:16:45.725 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:45.725 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:45.712 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 79 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899000693} from checkpoint 79
09:16:48.082 [main] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

09:16:48.595 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:16:48.596 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:16:50.695 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 80 @ 1595899010695 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:16:50.702 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 80 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 7 ms).
09:16:50.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 80 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899005699} from checkpoint 80
09:16:50.703 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 80 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899005702} from checkpoint 80
09:16:50.703 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 80 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899005702} from checkpoint 80
09:16:50.703 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 80 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899005700} from checkpoint 80
09:16:50.705 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:50.706 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:55.695 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 81 @ 1595899015695 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:16:55.699 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 81 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 4 ms).
09:16:55.699 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 81 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899010697} from checkpoint 81
09:16:55.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 81 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899010698} from checkpoint 81
09:16:55.699 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 81 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899010698} from checkpoint 81
09:16:55.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 81 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899010700} from checkpoint 81
09:16:55.702 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:16:55.702 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:00.696 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 82 @ 1595899020696 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:17:00.705 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 82 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 9 ms).
09:17:00.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 82 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899015696} from checkpoint 82
09:17:00.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 82 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899015696} from checkpoint 82
09:17:00.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 82 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899015696} from checkpoint 82
09:17:00.708 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 82 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899015696} from checkpoint 82
09:17:00.711 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:00.712 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:05.696 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 83 @ 1595899025696 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:17:05.705 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 83 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 9 ms).
09:17:05.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 83 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899020699} from checkpoint 83
09:17:05.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 83 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899020702} from checkpoint 83
09:17:05.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 83 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899020700} from checkpoint 83
09:17:05.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 83 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899020699} from checkpoint 83
09:17:05.708 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:05.708 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:10.696 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 84 @ 1595899030696 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:17:10.702 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 84 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:17:10.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 84 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899025698} from checkpoint 84
09:17:10.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 84 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899025700} from checkpoint 84
09:17:10.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 84 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899025699} from checkpoint 84
09:17:10.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 84 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899025698} from checkpoint 84
09:17:10.705 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:10.706 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:15.702 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 85 @ 1595899035702 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:17:15.713 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 85 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 11 ms).
09:17:15.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 85 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899030697} from checkpoint 85
09:17:15.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 85 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899030697} from checkpoint 85
09:17:15.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 85 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899030697} from checkpoint 85
09:17:15.714 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 85 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899030698} from checkpoint 85
09:17:15.714 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:15.715 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:15.746 [main] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

09:17:16.389 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
09:17:16.389 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
09:17:20.697 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 86 @ 1595899040697 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:17:20.705 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 86 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 8 ms).
09:17:20.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 86 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899035704} from checkpoint 86
09:17:20.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 86 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899035707} from checkpoint 86
09:17:20.707 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 86 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899035712} from checkpoint 86
09:17:20.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 86 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899035706} from checkpoint 86
09:17:20.708 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:20.710 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:25.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 87 @ 1595899045698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:17:25.705 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 87 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 7 ms).
09:17:25.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 87 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899040699} from checkpoint 87
09:17:25.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 87 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899040701} from checkpoint 87
09:17:25.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 87 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899040700} from checkpoint 87
09:17:25.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 87 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899040700} from checkpoint 87
09:17:25.709 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:25.710 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:26.543 [main] INFO  o.a.k.clients.producer.KafkaProducer - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
09:17:30.697 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 88 @ 1595899050697 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:17:30.704 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 88 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 7 ms).
09:17:30.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 88 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899045700} from checkpoint 88
09:17:30.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 88 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899045700} from checkpoint 88
09:17:30.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 88 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899045700} from checkpoint 88
09:17:30.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 88 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899045700} from checkpoint 88
09:17:30.709 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:30.710 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:35.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 89 @ 1595899055698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:17:35.702 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 89 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 3 ms).
09:17:35.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 89 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899050699} from checkpoint 89
09:17:35.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 89 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899050700} from checkpoint 89
09:17:35.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 89 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899050700} from checkpoint 89
09:17:35.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 89 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899050699} from checkpoint 89
09:17:35.703 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:35.704 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:40.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 90 @ 1595899060698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:17:40.700 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 90 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 2 ms).
09:17:40.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 90 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899055699} from checkpoint 90
09:17:40.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 90 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899055699} from checkpoint 90
09:17:40.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 90 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899055699} from checkpoint 90
09:17:40.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 90 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899055699} from checkpoint 90
09:17:40.701 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:40.702 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:45.697 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 91 @ 1595899065697 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:17:45.704 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 91 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 7 ms).
09:17:45.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 91 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899060698} from checkpoint 91
09:17:45.707 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 91 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899060699} from checkpoint 91
09:17:45.707 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 91 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899060698} from checkpoint 91
09:17:45.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 91 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899060698} from checkpoint 91
09:17:45.711 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:45.713 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:50.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 92 @ 1595899070698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:17:50.699 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 92 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 1 ms).
09:17:50.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 92 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899065699} from checkpoint 92
09:17:50.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 92 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899065699} from checkpoint 92
09:17:50.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 92 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899065699} from checkpoint 92
09:17:50.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 92 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899065700} from checkpoint 92
09:17:50.701 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:50.701 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:55.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 93 @ 1595899075699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:17:55.706 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 93 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:17:55.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 93 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899070698} from checkpoint 93
09:17:55.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 93 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899070698} from checkpoint 93
09:17:55.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 93 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899070698} from checkpoint 93
09:17:55.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 93 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899070698} from checkpoint 93
09:17:55.710 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:17:55.711 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:00.697 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 94 @ 1595899080697 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:18:00.704 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 94 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:18:00.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 94 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899075701} from checkpoint 94
09:18:00.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 94 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899075701} from checkpoint 94
09:18:00.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 94 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899075702} from checkpoint 94
09:18:00.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 94 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899075701} from checkpoint 94
09:18:00.709 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:00.710 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:05.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 95 @ 1595899085699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:18:05.703 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 95 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 4 ms).
09:18:05.703 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 95 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899080699} from checkpoint 95
09:18:05.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 95 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899080700} from checkpoint 95
09:18:05.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 95 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899080700} from checkpoint 95
09:18:05.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 95 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899080699} from checkpoint 95
09:18:05.706 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:05.707 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:10.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 96 @ 1595899090699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:18:10.701 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 96 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 2 ms).
09:18:10.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 96 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899085700} from checkpoint 96
09:18:10.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 96 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899085701} from checkpoint 96
09:18:10.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 96 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899085701} from checkpoint 96
09:18:10.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 96 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899085700} from checkpoint 96
09:18:10.702 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:10.702 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:15.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 97 @ 1595899095699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:18:15.701 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 97 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 2 ms).
09:18:15.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 97 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899090699} from checkpoint 97
09:18:15.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 97 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899090699} from checkpoint 97
09:18:15.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 97 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899090699} from checkpoint 97
09:18:15.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 97 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899090699} from checkpoint 97
09:18:15.703 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:15.703 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:20.708 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 98 @ 1595899100708 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:18:20.738 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 98 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 29 ms).
09:18:20.738 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 98 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899095699} from checkpoint 98
09:18:20.738 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 98 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899095699} from checkpoint 98
09:18:20.738 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 98 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899095699} from checkpoint 98
09:18:20.738 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 98 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899095699} from checkpoint 98
09:18:20.740 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:20.741 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:25.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 99 @ 1595899105699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:18:25.705 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 99 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:18:25.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 99 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899100734} from checkpoint 99
09:18:25.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 99 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899100736} from checkpoint 99
09:18:25.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 99 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899100736} from checkpoint 99
09:18:25.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 99 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899100735} from checkpoint 99
09:18:25.710 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:25.711 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:30.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 100 @ 1595899110698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:18:30.705 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 100 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 7 ms).
09:18:30.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 100 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899105701} from checkpoint 100
09:18:30.707 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 100 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899105701} from checkpoint 100
09:18:30.707 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 100 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899105701} from checkpoint 100
09:18:30.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 100 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899105701} from checkpoint 100
09:18:30.713 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:30.715 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:35.713 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 101 @ 1595899115713 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:18:35.718 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 101 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 5 ms).
09:18:35.718 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 101 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899110700} from checkpoint 101
09:18:35.718 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 101 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899110701} from checkpoint 101
09:18:35.718 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 101 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899110701} from checkpoint 101
09:18:35.718 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 101 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899110700} from checkpoint 101
09:18:35.719 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:35.720 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:40.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 102 @ 1595899120698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:18:40.706 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 102 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 8 ms).
09:18:40.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 102 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899115714} from checkpoint 102
09:18:40.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 102 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899115714} from checkpoint 102
09:18:40.707 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 102 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899115715} from checkpoint 102
09:18:40.707 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 102 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899115715} from checkpoint 102
09:18:40.710 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:40.719 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:45.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 103 @ 1595899125698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:18:45.699 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 103 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 1 ms).
09:18:45.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 103 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899120701} from checkpoint 103
09:18:45.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 103 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899120702} from checkpoint 103
09:18:45.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 103 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899120702} from checkpoint 103
09:18:45.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 103 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899120701} from checkpoint 103
09:18:45.701 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:45.701 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:50.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 104 @ 1595899130698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:18:50.706 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 104 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 8 ms).
09:18:50.709 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 104 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899125698} from checkpoint 104
09:18:50.709 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 104 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899125698} from checkpoint 104
09:18:50.708 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 104 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899125698} from checkpoint 104
09:18:50.709 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 104 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899125698} from checkpoint 104
09:18:50.714 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:50.716 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:55.697 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 105 @ 1595899135697 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:18:55.703 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 105 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:18:55.703 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 105 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899130700} from checkpoint 105
09:18:55.703 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 105 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899130701} from checkpoint 105
09:18:55.703 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 105 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899130702} from checkpoint 105
09:18:55.703 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 105 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899130701} from checkpoint 105
09:18:55.707 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:18:55.709 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:00.697 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 106 @ 1595899140697 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:19:00.709 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 106 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 11 ms).
09:19:00.710 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 106 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899135699} from checkpoint 106
09:19:00.710 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 106 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899135699} from checkpoint 106
09:19:00.710 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 106 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899135699} from checkpoint 106
09:19:00.709 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 106 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899135699} from checkpoint 106
09:19:00.715 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:00.716 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:05.697 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 107 @ 1595899145697 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:19:05.703 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 107 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:19:05.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 107 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899140699} from checkpoint 107
09:19:05.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 107 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899140701} from checkpoint 107
09:19:05.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 107 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899140699} from checkpoint 107
09:19:05.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 107 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899140700} from checkpoint 107
09:19:05.708 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:05.710 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:10.697 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 108 @ 1595899150697 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:19:10.704 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 108 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:19:10.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 108 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899145699} from checkpoint 108
09:19:10.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 108 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899145700} from checkpoint 108
09:19:10.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 108 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899145700} from checkpoint 108
09:19:10.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 108 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899145699} from checkpoint 108
09:19:10.709 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:10.711 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:15.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 109 @ 1595899155699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:19:15.706 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 109 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 7 ms).
09:19:15.707 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 109 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899150699} from checkpoint 109
09:19:15.707 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 109 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899150700} from checkpoint 109
09:19:15.707 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 109 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899150700} from checkpoint 109
09:19:15.707 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 109 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899150699} from checkpoint 109
09:19:15.712 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:15.713 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:20.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 110 @ 1595899160699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:19:20.704 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 110 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 5 ms).
09:19:20.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 110 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899155701} from checkpoint 110
09:19:20.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 110 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899155702} from checkpoint 110
09:19:20.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 110 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899155702} from checkpoint 110
09:19:20.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 110 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899155701} from checkpoint 110
09:19:20.708 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:20.708 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:25.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 111 @ 1595899165698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:19:25.705 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 111 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:19:25.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 111 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899160700} from checkpoint 111
09:19:25.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 111 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899160701} from checkpoint 111
09:19:25.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 111 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899160700} from checkpoint 111
09:19:25.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 111 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899160700} from checkpoint 111
09:19:25.710 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:25.712 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:30.697 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 112 @ 1595899170697 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:19:30.703 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 112 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:19:30.703 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 112 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899165700} from checkpoint 112
09:19:30.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 112 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899165701} from checkpoint 112
09:19:30.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 112 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899165701} from checkpoint 112
09:19:30.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 112 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899165700} from checkpoint 112
09:19:30.707 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:30.709 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:35.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 113 @ 1595899175699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:19:35.710 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 113 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 11 ms).
09:19:35.711 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 113 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899170699} from checkpoint 113
09:19:35.712 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 113 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899170700} from checkpoint 113
09:19:35.711 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 113 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899170699} from checkpoint 113
09:19:35.711 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 113 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899170699} from checkpoint 113
09:19:35.715 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:35.717 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:40.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 114 @ 1595899180698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:19:40.699 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 114 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 1 ms).
09:19:40.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 114 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899175701} from checkpoint 114
09:19:40.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 114 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899175703} from checkpoint 114
09:19:40.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 114 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899175701} from checkpoint 114
09:19:40.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 114 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899175701} from checkpoint 114
09:19:40.701 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:40.701 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:45.697 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 115 @ 1595899185697 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:19:45.699 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 115 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 2 ms).
09:19:45.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 115 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899180698} from checkpoint 115
09:19:45.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 115 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899180698} from checkpoint 115
09:19:45.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 115 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899180698} from checkpoint 115
09:19:45.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 115 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899180698} from checkpoint 115
09:19:45.702 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:45.702 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:50.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 116 @ 1595899190699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:19:50.703 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 116 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 4 ms).
09:19:50.703 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 116 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899185698} from checkpoint 116
09:19:50.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 116 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899185698} from checkpoint 116
09:19:50.703 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 116 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899185698} from checkpoint 116
09:19:50.703 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 116 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899185698} from checkpoint 116
09:19:50.706 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:50.707 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:55.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 117 @ 1595899195698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:19:55.704 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 117 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:19:55.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 117 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899190700} from checkpoint 117
09:19:55.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 117 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899190700} from checkpoint 117
09:19:55.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 117 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899190701} from checkpoint 117
09:19:55.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 117 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899190701} from checkpoint 117
09:19:55.708 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:19:55.710 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:00.697 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 118 @ 1595899200697 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:20:00.701 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 118 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 4 ms).
09:20:00.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 118 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899195700} from checkpoint 118
09:20:00.703 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 118 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899195699} from checkpoint 118
09:20:00.703 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 118 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899195700} from checkpoint 118
09:20:00.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 118 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899195700} from checkpoint 118
09:20:00.707 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:00.707 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:05.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 119 @ 1595899205699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:20:05.703 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 119 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 4 ms).
09:20:05.703 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 119 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899200697} from checkpoint 119
09:20:05.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 119 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899200698} from checkpoint 119
09:20:05.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 119 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899200698} from checkpoint 119
09:20:05.703 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 119 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899200698} from checkpoint 119
09:20:05.706 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:05.707 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:10.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 120 @ 1595899210698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:20:10.704 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 120 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:20:10.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 120 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899205700} from checkpoint 120
09:20:10.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 120 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899205700} from checkpoint 120
09:20:10.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 120 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899205700} from checkpoint 120
09:20:10.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 120 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899205701} from checkpoint 120
09:20:10.710 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:10.711 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:15.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 121 @ 1595899215698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:20:15.701 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 121 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 3 ms).
09:20:15.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 121 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899210700} from checkpoint 121
09:20:15.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 121 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899210701} from checkpoint 121
09:20:15.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 121 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899210700} from checkpoint 121
09:20:15.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 121 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899210700} from checkpoint 121
09:20:15.703 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:15.703 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:20.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 122 @ 1595899220699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:20:20.703 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 122 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 4 ms).
09:20:20.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 122 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899215700} from checkpoint 122
09:20:20.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 122 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899215700} from checkpoint 122
09:20:20.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 122 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899215700} from checkpoint 122
09:20:20.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 122 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899215700} from checkpoint 122
09:20:20.706 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:20.707 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:25.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 123 @ 1595899225698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:20:25.704 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 123 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:20:25.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 123 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899220700} from checkpoint 123
09:20:25.707 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 123 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899220701} from checkpoint 123
09:20:25.708 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 123 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899220701} from checkpoint 123
09:20:25.708 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 123 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899220701} from checkpoint 123
09:20:25.711 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:25.712 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:30.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 124 @ 1595899230699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:20:30.703 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 124 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 4 ms).
09:20:30.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 124 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899225700} from checkpoint 124
09:20:30.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 124 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899225700} from checkpoint 124
09:20:30.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 124 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899225700} from checkpoint 124
09:20:30.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 124 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899225700} from checkpoint 124
09:20:30.706 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:30.707 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:35.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 125 @ 1595899235699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:20:35.701 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 125 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 2 ms).
09:20:35.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 125 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899230700} from checkpoint 125
09:20:35.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 125 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899230700} from checkpoint 125
09:20:35.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 125 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899230700} from checkpoint 125
09:20:35.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 125 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899230700} from checkpoint 125
09:20:35.704 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:35.704 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:40.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 126 @ 1595899240699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:20:40.704 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 126 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 5 ms).
09:20:40.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 126 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899235699} from checkpoint 126
09:20:40.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 126 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899235699} from checkpoint 126
09:20:40.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 126 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899235700} from checkpoint 126
09:20:40.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 126 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899235699} from checkpoint 126
09:20:40.707 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:40.708 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:45.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 127 @ 1595899245699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:20:45.701 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 127 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 2 ms).
09:20:45.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 127 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899240701} from checkpoint 127
09:20:45.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 127 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899240701} from checkpoint 127
09:20:45.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 127 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899240701} from checkpoint 127
09:20:45.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 127 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899240701} from checkpoint 127
09:20:45.702 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:45.702 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:50.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 128 @ 1595899250699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:20:50.704 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 128 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 5 ms).
09:20:50.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 128 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899245699} from checkpoint 128
09:20:50.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 128 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899245699} from checkpoint 128
09:20:50.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 128 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899245699} from checkpoint 128
09:20:50.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 128 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899245699} from checkpoint 128
09:20:50.710 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:50.711 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:55.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 129 @ 1595899255699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:20:55.705 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 129 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:20:55.709 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 129 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899250700} from checkpoint 129
09:20:55.710 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 129 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899250701} from checkpoint 129
09:20:55.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 129 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899250700} from checkpoint 129
09:20:55.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 129 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899250700} from checkpoint 129
09:20:55.717 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:20:55.717 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:00.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 130 @ 1595899260698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:21:00.701 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 130 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 3 ms).
09:21:00.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 130 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899255701} from checkpoint 130
09:21:00.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 130 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899255702} from checkpoint 130
09:21:00.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 130 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899255701} from checkpoint 130
09:21:00.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 130 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899255701} from checkpoint 130
09:21:00.703 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:00.704 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:05.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 131 @ 1595899265699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:21:05.706 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 131 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 7 ms).
09:21:05.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 131 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899260699} from checkpoint 131
09:21:05.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 131 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899260699} from checkpoint 131
09:21:05.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 131 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899260699} from checkpoint 131
09:21:05.706 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 131 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899260699} from checkpoint 131
09:21:05.712 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:05.713 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:10.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 132 @ 1595899270699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:21:10.701 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 132 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 2 ms).
09:21:10.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 132 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899265701} from checkpoint 132
09:21:10.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 132 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899265702} from checkpoint 132
09:21:10.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 132 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899265702} from checkpoint 132
09:21:10.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 132 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899265702} from checkpoint 132
09:21:10.704 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:10.705 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:15.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 133 @ 1595899275698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:21:15.699 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 133 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 1 ms).
09:21:15.699 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 133 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899270700} from checkpoint 133
09:21:15.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 133 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899270700} from checkpoint 133
09:21:15.700 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 133 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899270700} from checkpoint 133
09:21:15.699 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 133 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899270700} from checkpoint 133
09:21:15.701 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:15.701 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:20.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 134 @ 1595899280698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:21:20.704 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 134 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 5 ms).
09:21:20.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 134 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899275698} from checkpoint 134
09:21:20.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 134 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899275698} from checkpoint 134
09:21:20.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 134 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899275698} from checkpoint 134
09:21:20.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 134 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899275698} from checkpoint 134
09:21:20.708 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:20.710 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:25.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 135 @ 1595899285698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:21:25.701 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 135 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 3 ms).
09:21:25.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 135 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899280700} from checkpoint 135
09:21:25.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 135 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899280701} from checkpoint 135
09:21:25.702 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 135 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899280700} from checkpoint 135
09:21:25.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 135 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899280700} from checkpoint 135
09:21:25.704 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:25.705 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:30.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 136 @ 1595899290698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:21:30.701 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 136 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 3 ms).
09:21:30.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 136 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899285699} from checkpoint 136
09:21:30.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 136 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899285699} from checkpoint 136
09:21:30.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 136 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899285699} from checkpoint 136
09:21:30.701 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 136 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899285699} from checkpoint 136
09:21:30.704 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:30.704 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:35.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 137 @ 1595899295698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:21:35.704 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 137 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:21:35.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 137 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899290699} from checkpoint 137
09:21:35.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 137 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899290699} from checkpoint 137
09:21:35.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 137 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899290699} from checkpoint 137
09:21:35.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 137 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899290699} from checkpoint 137
09:21:35.709 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:35.711 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:40.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 138 @ 1595899300698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:21:40.704 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 138 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 6 ms).
09:21:40.704 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 138 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899295700} from checkpoint 138
09:21:40.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 138 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899295700} from checkpoint 138
09:21:40.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 138 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899295701} from checkpoint 138
09:21:40.705 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 138 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899295700} from checkpoint 138
09:21:40.709 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:40.710 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:45.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 139 @ 1595899305698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:21:45.741 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 139 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 43 ms).
09:21:45.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 139 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899300700} from checkpoint 139
09:21:45.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 139 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899300700} from checkpoint 139
09:21:45.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 139 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899300700} from checkpoint 139
09:21:45.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 139 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899300700} from checkpoint 139
09:21:45.761 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:45.774 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:50.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 140 @ 1595899310698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:21:50.742 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 140 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 44 ms).
09:21:50.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 140 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899305739} from checkpoint 140
09:21:50.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 140 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899305740} from checkpoint 140
09:21:50.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 140 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899305739} from checkpoint 140
09:21:50.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 140 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899305739} from checkpoint 140
09:21:50.768 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:50.791 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:55.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 141 @ 1595899315698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:21:55.712 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 141 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 14 ms).
09:21:55.712 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 141 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899310740} from checkpoint 141
09:21:55.712 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 141 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899310740} from checkpoint 141
09:21:55.712 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 141 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899310740} from checkpoint 141
09:21:55.712 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 141 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899310740} from checkpoint 141
09:21:55.726 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:21:55.738 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:00.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 142 @ 1595899320698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:22:00.726 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 142 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 28 ms).
09:22:00.726 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 142 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899315710} from checkpoint 142
09:22:00.727 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 142 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899315711} from checkpoint 142
09:22:00.726 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 142 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899315711} from checkpoint 142
09:22:00.726 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 142 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899315710} from checkpoint 142
09:22:00.740 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:00.754 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:05.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 143 @ 1595899325698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:22:05.718 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 143 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 20 ms).
09:22:05.719 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 143 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899320724} from checkpoint 143
09:22:05.719 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 143 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899320725} from checkpoint 143
09:22:05.719 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 143 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899320725} from checkpoint 143
09:22:05.719 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 143 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899320724} from checkpoint 143
09:22:05.733 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:05.746 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:10.697 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 144 @ 1595899330697 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:22:10.760 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 144 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 63 ms).
09:22:10.761 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 144 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899325717} from checkpoint 144
09:22:10.762 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 144 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899325717} from checkpoint 144
09:22:10.762 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 144 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899325717} from checkpoint 144
09:22:10.761 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 144 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899325717} from checkpoint 144
09:22:10.800 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:10.817 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:15.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 145 @ 1595899335698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:22:15.742 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 145 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 43 ms).
09:22:15.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 145 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899330757} from checkpoint 145
09:22:15.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 145 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899330757} from checkpoint 145
09:22:15.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 145 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899330757} from checkpoint 145
09:22:15.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 145 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899330756} from checkpoint 145
09:22:15.763 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:15.777 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:20.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 146 @ 1595899340699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:22:20.759 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 146 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 60 ms).
09:22:20.760 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 146 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899335739} from checkpoint 146
09:22:20.761 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 146 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899335740} from checkpoint 146
09:22:20.761 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 146 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899335740} from checkpoint 146
09:22:20.761 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 146 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899335740} from checkpoint 146
09:22:20.796 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:20.815 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:25.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 147 @ 1595899345698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:22:25.751 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 147 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 53 ms).
09:22:25.752 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 147 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899340756} from checkpoint 147
09:22:25.752 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 147 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899340756} from checkpoint 147
09:22:25.752 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 147 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899340756} from checkpoint 147
09:22:25.752 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 147 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899340755} from checkpoint 147
09:22:25.776 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:25.788 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:30.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 148 @ 1595899350698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:22:30.732 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 148 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 34 ms).
09:22:30.732 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 148 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899345750} from checkpoint 148
09:22:30.733 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 148 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899345750} from checkpoint 148
09:22:30.733 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 148 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899345750} from checkpoint 148
09:22:30.732 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 148 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899345750} from checkpoint 148
09:22:30.754 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:30.767 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:35.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 149 @ 1595899355699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:22:35.713 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 149 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 13 ms).
09:22:35.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 149 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899350731} from checkpoint 149
09:22:35.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 149 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899350731} from checkpoint 149
09:22:35.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 149 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899350731} from checkpoint 149
09:22:35.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 149 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899350731} from checkpoint 149
09:22:35.726 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:35.738 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:40.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 150 @ 1595899360698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:22:40.751 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 150 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 53 ms).
09:22:40.752 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 150 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899355711} from checkpoint 150
09:22:40.752 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 150 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899355712} from checkpoint 150
09:22:40.752 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 150 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899355712} from checkpoint 150
09:22:40.752 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 150 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899355711} from checkpoint 150
09:22:40.778 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:40.792 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:45.728 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 151 @ 1595899365728 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:22:45.769 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 151 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 41 ms).
09:22:45.771 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 151 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899360749} from checkpoint 151
09:22:45.771 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 151 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899360750} from checkpoint 151
09:22:45.771 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 151 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899360750} from checkpoint 151
09:22:45.771 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 151 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899360749} from checkpoint 151
09:22:45.785 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:45.799 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:50.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 152 @ 1595899370698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:22:50.758 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 152 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 60 ms).
09:22:50.759 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 152 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899365768} from checkpoint 152
09:22:50.760 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 152 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899365768} from checkpoint 152
09:22:50.761 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 152 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899365768} from checkpoint 152
09:22:50.760 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 152 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899365768} from checkpoint 152
09:22:50.788 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:50.804 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:55.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 153 @ 1595899375699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:22:55.715 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 153 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 16 ms).
09:22:55.716 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 153 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899370754} from checkpoint 153
09:22:55.716 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 153 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899370755} from checkpoint 153
09:22:55.716 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 153 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899370755} from checkpoint 153
09:22:55.716 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 153 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899370754} from checkpoint 153
09:22:55.730 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:22:55.742 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:00.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 154 @ 1595899380699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:23:00.744 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 154 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 45 ms).
09:23:00.745 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 154 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899375714} from checkpoint 154
09:23:00.745 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 154 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899375714} from checkpoint 154
09:23:00.745 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 154 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899375714} from checkpoint 154
09:23:00.745 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 154 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899375714} from checkpoint 154
09:23:00.768 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:00.786 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:05.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 155 @ 1595899385698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:23:05.728 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 155 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 30 ms).
09:23:05.728 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 155 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899380742} from checkpoint 155
09:23:05.729 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 155 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899380743} from checkpoint 155
09:23:05.729 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 155 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899380743} from checkpoint 155
09:23:05.728 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 155 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899380742} from checkpoint 155
09:23:05.746 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:05.759 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:10.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 156 @ 1595899390698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:23:10.712 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 156 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 14 ms).
09:23:10.712 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 156 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899385726} from checkpoint 156
09:23:10.712 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 156 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899385726} from checkpoint 156
09:23:10.712 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 156 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899385726} from checkpoint 156
09:23:10.712 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 156 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899385726} from checkpoint 156
09:23:10.725 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:10.738 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:15.700 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 157 @ 1595899395700 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:23:15.713 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 157 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 13 ms).
09:23:15.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 157 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899390711} from checkpoint 157
09:23:15.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 157 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899390711} from checkpoint 157
09:23:15.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 157 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899390711} from checkpoint 157
09:23:15.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 157 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899390711} from checkpoint 157
09:23:15.728 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:15.739 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:20.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 158 @ 1595899400698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:23:20.741 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 158 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 43 ms).
09:23:20.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 158 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899395713} from checkpoint 158
09:23:20.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 158 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899395713} from checkpoint 158
09:23:20.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 158 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899395713} from checkpoint 158
09:23:20.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 158 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899395713} from checkpoint 158
09:23:20.766 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:20.778 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:25.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 159 @ 1595899405699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:23:25.720 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 159 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 21 ms).
09:23:25.721 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 159 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899400740} from checkpoint 159
09:23:25.721 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 159 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899400740} from checkpoint 159
09:23:25.721 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 159 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899400740} from checkpoint 159
09:23:25.721 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 159 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899400740} from checkpoint 159
09:23:25.743 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:25.757 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:30.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 160 @ 1595899410699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:23:30.715 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 160 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 16 ms).
09:23:30.716 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 160 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899405719} from checkpoint 160
09:23:30.717 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 160 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899405719} from checkpoint 160
09:23:30.717 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 160 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899405719} from checkpoint 160
09:23:30.717 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 160 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899405719} from checkpoint 160
09:23:30.731 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:30.743 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:35.701 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 161 @ 1595899415701 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:23:35.719 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 161 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 18 ms).
09:23:35.720 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 161 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899410713} from checkpoint 161
09:23:35.720 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 161 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899410714} from checkpoint 161
09:23:35.720 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 161 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899410713} from checkpoint 161
09:23:35.720 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 161 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899410713} from checkpoint 161
09:23:35.734 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:35.747 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:40.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 162 @ 1595899420698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:23:40.758 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 162 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 60 ms).
09:23:40.759 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 162 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899415718} from checkpoint 162
09:23:40.761 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 162 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899415718} from checkpoint 162
09:23:40.760 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 162 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899415718} from checkpoint 162
09:23:40.759 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 162 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899415718} from checkpoint 162
09:23:40.796 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:40.822 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:45.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 163 @ 1595899425699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:23:45.715 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 163 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 15 ms).
09:23:45.715 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 163 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899420754} from checkpoint 163
09:23:45.716 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 163 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899420755} from checkpoint 163
09:23:45.716 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 163 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899420754} from checkpoint 163
09:23:45.715 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 163 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899420754} from checkpoint 163
09:23:45.729 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:45.742 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:50.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 164 @ 1595899430699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:23:50.732 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 164 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 33 ms).
09:23:50.732 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 164 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899425713} from checkpoint 164
09:23:50.732 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 164 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899425713} from checkpoint 164
09:23:50.732 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 164 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899425713} from checkpoint 164
09:23:50.732 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 164 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899425713} from checkpoint 164
09:23:50.751 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:50.767 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:55.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 165 @ 1595899435698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:23:55.730 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 165 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 32 ms).
09:23:55.731 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 165 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899430729} from checkpoint 165
09:23:55.731 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 165 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899430729} from checkpoint 165
09:23:55.731 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 165 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899430730} from checkpoint 165
09:23:55.731 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 165 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899430729} from checkpoint 165
09:23:55.755 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:23:55.772 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:00.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 166 @ 1595899440699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:24:00.714 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 166 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 15 ms).
09:24:00.717 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 166 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899435728} from checkpoint 166
09:24:00.718 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 166 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899435728} from checkpoint 166
09:24:00.718 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 166 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899435728} from checkpoint 166
09:24:00.718 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 166 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899435729} from checkpoint 166
09:24:00.731 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:00.743 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:05.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 167 @ 1595899445698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:24:05.746 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 167 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 48 ms).
09:24:05.747 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 167 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899440712} from checkpoint 167
09:24:05.748 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 167 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899440713} from checkpoint 167
09:24:05.748 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 167 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899440713} from checkpoint 167
09:24:05.747 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 167 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899440713} from checkpoint 167
09:24:05.770 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:05.782 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:10.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 168 @ 1595899450698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:24:10.729 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 168 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 31 ms).
09:24:10.732 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 168 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899445744} from checkpoint 168
09:24:10.733 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 168 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899445744} from checkpoint 168
09:24:10.733 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 168 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899445744} from checkpoint 168
09:24:10.733 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 168 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899445744} from checkpoint 168
09:24:10.762 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:10.775 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:15.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 169 @ 1595899455698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:24:15.742 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 169 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 44 ms).
09:24:15.743 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 169 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899450722} from checkpoint 169
09:24:15.743 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 169 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899450723} from checkpoint 169
09:24:15.743 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 169 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899450723} from checkpoint 169
09:24:15.743 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 169 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899450722} from checkpoint 169
09:24:15.770 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:15.796 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:20.700 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 170 @ 1595899460700 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:24:20.713 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 170 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 13 ms).
09:24:20.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 170 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899455739} from checkpoint 170
09:24:20.714 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 170 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899455740} from checkpoint 170
09:24:20.714 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 170 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899455740} from checkpoint 170
09:24:20.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 170 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899455740} from checkpoint 170
09:24:20.726 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:20.738 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:25.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 171 @ 1595899465698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:24:25.736 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 171 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 38 ms).
09:24:25.737 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 171 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899460712} from checkpoint 171
09:24:25.737 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 171 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899460712} from checkpoint 171
09:24:25.737 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 171 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899460712} from checkpoint 171
09:24:25.737 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 171 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899460712} from checkpoint 171
09:24:25.755 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:25.769 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:30.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 172 @ 1595899470699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:24:30.713 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 172 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 14 ms).
09:24:30.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 172 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899465734} from checkpoint 172
09:24:30.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 172 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899465735} from checkpoint 172
09:24:30.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 172 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899465735} from checkpoint 172
09:24:30.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 172 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899465734} from checkpoint 172
09:24:30.726 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:30.738 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:35.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 173 @ 1595899475698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:24:35.748 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 173 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 50 ms).
09:24:35.749 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 173 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899470712} from checkpoint 173
09:24:35.749 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 173 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899470712} from checkpoint 173
09:24:35.749 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 173 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899470712} from checkpoint 173
09:24:35.749 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 173 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899470712} from checkpoint 173
09:24:35.774 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:35.789 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:40.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 174 @ 1595899480699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:24:40.753 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 174 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 54 ms).
09:24:40.753 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 174 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899475746} from checkpoint 174
09:24:40.754 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 174 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899475746} from checkpoint 174
09:24:40.754 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 174 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899475746} from checkpoint 174
09:24:40.754 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 174 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899475746} from checkpoint 174
09:24:40.781 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:40.797 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:45.709 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 175 @ 1595899485708 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:24:45.724 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 175 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 16 ms).
09:24:45.724 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 175 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899480751} from checkpoint 175
09:24:45.724 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 175 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899480751} from checkpoint 175
09:24:45.724 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 175 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899480751} from checkpoint 175
09:24:45.724 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 175 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899480750} from checkpoint 175
09:24:45.737 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:45.750 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:50.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 176 @ 1595899490698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:24:50.750 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 176 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 51 ms).
09:24:50.750 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 176 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899485722} from checkpoint 176
09:24:50.750 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 176 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899485723} from checkpoint 176
09:24:50.750 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 176 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899485723} from checkpoint 176
09:24:50.750 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 176 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899485722} from checkpoint 176
09:24:50.773 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:50.787 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:55.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 177 @ 1595899495698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:24:55.739 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 177 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 41 ms).
09:24:55.740 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 177 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899490747} from checkpoint 177
09:24:55.740 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 177 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899490747} from checkpoint 177
09:24:55.740 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 177 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899490747} from checkpoint 177
09:24:55.740 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 177 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899490747} from checkpoint 177
09:24:55.763 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:24:55.779 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:00.700 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 178 @ 1595899500700 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:25:00.713 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 178 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 13 ms).
09:25:00.714 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 178 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899495737} from checkpoint 178
09:25:00.715 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 178 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899495737} from checkpoint 178
09:25:00.714 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 178 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899495737} from checkpoint 178
09:25:00.714 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 178 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899495737} from checkpoint 178
09:25:00.729 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:00.741 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:05.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 179 @ 1595899505699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:25:05.747 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 179 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 48 ms).
09:25:05.747 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 179 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899500712} from checkpoint 179
09:25:05.747 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 179 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899500712} from checkpoint 179
09:25:05.747 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 179 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899500712} from checkpoint 179
09:25:05.747 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 179 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899500712} from checkpoint 179
09:25:05.770 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:05.784 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:10.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 180 @ 1595899510699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:25:10.726 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 180 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 27 ms).
09:25:10.784 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 180 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899505745} from checkpoint 180
09:25:10.836 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 180 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899505745} from checkpoint 180
09:25:10.836 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 180 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899505745} from checkpoint 180
09:25:10.835 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 180 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899505745} from checkpoint 180
09:25:10.849 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:10.861 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:15.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 181 @ 1595899515699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:25:15.714 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 181 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 15 ms).
09:25:15.714 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 181 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899510721} from checkpoint 181
09:25:15.715 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 181 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899510722} from checkpoint 181
09:25:15.714 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 181 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899510722} from checkpoint 181
09:25:15.714 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 181 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899510721} from checkpoint 181
09:25:15.734 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:15.747 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:20.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 182 @ 1595899520698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:25:20.748 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 182 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 50 ms).
09:25:20.748 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 182 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899515713} from checkpoint 182
09:25:20.748 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 182 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899515713} from checkpoint 182
09:25:20.748 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 182 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899515713} from checkpoint 182
09:25:20.748 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 182 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899515713} from checkpoint 182
09:25:20.775 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:20.791 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:25.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 183 @ 1595899525698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:25:25.741 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 183 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 43 ms).
09:25:25.746 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 183 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899520746} from checkpoint 183
09:25:25.746 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 183 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899520746} from checkpoint 183
09:25:25.746 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 183 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899520746} from checkpoint 183
09:25:25.746 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 183 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899520746} from checkpoint 183
09:25:25.762 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:25.776 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:30.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 184 @ 1595899530699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:25:30.744 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 184 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 45 ms).
09:25:30.745 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 184 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899525739} from checkpoint 184
09:25:30.745 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 184 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899525740} from checkpoint 184
09:25:30.745 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 184 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899525740} from checkpoint 184
09:25:30.745 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 184 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899525739} from checkpoint 184
09:25:30.768 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:30.786 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:35.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 185 @ 1595899535698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:25:35.765 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 185 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 67 ms).
09:25:35.766 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 185 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899530742} from checkpoint 185
09:25:35.767 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 185 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899530742} from checkpoint 185
09:25:35.767 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 185 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899530742} from checkpoint 185
09:25:35.767 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 185 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899530742} from checkpoint 185
09:25:35.789 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:35.804 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:40.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 186 @ 1595899540698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:25:40.731 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 186 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 33 ms).
09:25:40.732 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 186 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899535758} from checkpoint 186
09:25:40.732 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 186 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899535761} from checkpoint 186
09:25:40.732 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 186 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899535761} from checkpoint 186
09:25:40.732 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 186 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899535760} from checkpoint 186
09:25:40.764 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:40.795 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:45.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 187 @ 1595899545698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:25:45.711 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 187 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 13 ms).
09:25:45.711 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 187 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899540729} from checkpoint 187
09:25:45.712 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 187 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899540729} from checkpoint 187
09:25:45.712 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 187 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899540729} from checkpoint 187
09:25:45.712 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 187 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899540729} from checkpoint 187
09:25:45.725 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:45.737 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:50.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 188 @ 1595899550698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:25:50.738 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 188 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 40 ms).
09:25:50.738 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 188 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899545710} from checkpoint 188
09:25:50.739 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 188 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899545710} from checkpoint 188
09:25:50.738 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 188 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899545710} from checkpoint 188
09:25:50.738 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 188 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899545710} from checkpoint 188
09:25:50.755 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:50.770 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:55.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 189 @ 1595899555698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:25:55.726 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 189 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 28 ms).
09:25:55.726 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 189 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899550736} from checkpoint 189
09:25:55.726 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 189 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899550736} from checkpoint 189
09:25:55.726 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 189 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899550736} from checkpoint 189
09:25:55.726 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 189 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899550736} from checkpoint 189
09:25:55.747 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:25:55.765 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:00.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 190 @ 1595899560699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:26:00.727 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 190 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 28 ms).
09:26:00.727 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 190 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899555724} from checkpoint 190
09:26:00.727 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 190 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899555724} from checkpoint 190
09:26:00.727 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 190 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899555724} from checkpoint 190
09:26:00.727 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 190 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899555724} from checkpoint 190
09:26:00.743 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:00.757 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:05.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 191 @ 1595899565698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:26:05.731 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 191 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 33 ms).
09:26:05.732 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 191 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899560725} from checkpoint 191
09:26:05.733 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 191 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899560725} from checkpoint 191
09:26:05.733 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 191 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899560725} from checkpoint 191
09:26:05.733 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 191 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899560725} from checkpoint 191
09:26:05.750 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:05.764 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:10.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 192 @ 1595899570698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:26:10.755 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 192 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 57 ms).
09:26:10.756 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 192 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899565729} from checkpoint 192
09:26:10.757 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 192 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899565729} from checkpoint 192
09:26:10.757 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 192 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899565729} from checkpoint 192
09:26:10.757 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 192 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899565729} from checkpoint 192
09:26:10.781 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:10.793 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:15.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 193 @ 1595899575698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:26:15.744 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 193 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 46 ms).
09:26:15.745 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 193 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899570753} from checkpoint 193
09:26:15.746 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 193 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899570753} from checkpoint 193
09:26:15.746 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 193 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899570753} from checkpoint 193
09:26:15.745 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 193 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899570752} from checkpoint 193
09:26:15.765 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:15.777 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:20.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 194 @ 1595899580699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:26:20.758 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 194 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 59 ms).
09:26:20.759 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 194 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899575741} from checkpoint 194
09:26:20.760 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 194 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899575741} from checkpoint 194
09:26:20.759 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 194 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899575741} from checkpoint 194
09:26:20.759 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 194 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899575741} from checkpoint 194
09:26:20.805 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:20.827 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:25.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 195 @ 1595899585699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:26:25.740 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 195 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 41 ms).
09:26:25.741 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 195 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899580754} from checkpoint 195
09:26:25.741 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 195 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899580756} from checkpoint 195
09:26:25.741 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 195 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899580755} from checkpoint 195
09:26:25.741 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 195 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899580754} from checkpoint 195
09:26:25.760 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:25.774 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:30.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 196 @ 1595899590699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:26:30.757 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 196 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 58 ms).
09:26:30.758 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 196 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899585738} from checkpoint 196
09:26:30.758 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 196 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899585738} from checkpoint 196
09:26:30.758 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 196 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899585738} from checkpoint 196
09:26:30.758 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 196 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899585738} from checkpoint 196
09:26:30.782 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:30.798 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:35.699 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 197 @ 1595899595699 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:26:35.730 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 197 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 31 ms).
09:26:35.731 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 197 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899590755} from checkpoint 197
09:26:35.731 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 197 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899590755} from checkpoint 197
09:26:35.731 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 197 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899590755} from checkpoint 197
09:26:35.731 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 197 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899590755} from checkpoint 197
09:26:35.746 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:35.759 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:40.700 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 198 @ 1595899600700 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:26:40.713 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 198 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 13 ms).
09:26:40.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 198 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899595728} from checkpoint 198
09:26:40.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 198 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899595729} from checkpoint 198
09:26:40.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 198 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899595728} from checkpoint 198
09:26:40.713 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 198 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899595728} from checkpoint 198
09:26:40.726 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:40.739 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:45.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 199 @ 1595899605698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:26:45.758 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 199 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 60 ms).
09:26:45.759 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 199 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899600712} from checkpoint 199
09:26:45.759 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 199 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899600712} from checkpoint 199
09:26:45.759 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 199 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899600712} from checkpoint 199
09:26:45.759 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 199 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899600712} from checkpoint 199
09:26:45.788 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:45.803 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:50.712 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 200 @ 1595899610710 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:26:50.740 [jobmanager-future-thread-2] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 200 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 30 ms).
09:26:50.740 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 200 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899605755} from checkpoint 200
09:26:50.740 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 200 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899605757} from checkpoint 200
09:26:50.740 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 200 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899605757} from checkpoint 200
09:26:50.740 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 200 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899605756} from checkpoint 200
09:26:50.755 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:50.767 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:55.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 201 @ 1595899615698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:26:55.741 [jobmanager-future-thread-3] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 201 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 43 ms).
09:26:55.741 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 201 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899610739} from checkpoint 201
09:26:55.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 201 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899610739} from checkpoint 201
09:26:55.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 201 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899610739} from checkpoint 201
09:26:55.741 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 201 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899610739} from checkpoint 201
09:26:55.762 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:26:55.778 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:27:00.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 202 @ 1595899620698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:27:00.742 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 202 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 44 ms).
09:27:00.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 202 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899615738} from checkpoint 202
09:27:00.743 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 202 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899615739} from checkpoint 202
09:27:00.743 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 202 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899615739} from checkpoint 202
09:27:00.742 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 202 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899615739} from checkpoint 202
09:27:00.764 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:27:00.783 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:27:05.698 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 203 @ 1595899625698 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:27:05.747 [jobmanager-future-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 203 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 49 ms).
09:27:05.748 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 203 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899620740} from checkpoint 203
09:27:05.748 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 203 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899620740} from checkpoint 203
09:27:05.748 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 203 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899620741} from checkpoint 203
09:27:05.748 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 203 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899620740} from checkpoint 203
09:27:05.773 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:27:05.788 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:27:10.700 [Checkpoint Timer] INFO  o.a.f.r.c.CheckpointCoordinator - Triggering checkpoint 204 @ 1595899630700 for job f4e075b4f58efd9c9f7f06e943b4e443.
09:27:10.724 [jobmanager-future-thread-4] INFO  o.a.f.r.c.CheckpointCoordinator - Completed checkpoint 204 for job f4e075b4f58efd9c9f7f06e943b4e443 (10996 bytes in 24 ms).
09:27:10.724 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (1/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 0/4 - checkpoint 204 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899625746} from checkpoint 204
09:27:10.724 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (4/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 3/4 - checkpoint 204 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899625746} from checkpoint 204
09:27:10.724 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 2/4 - checkpoint 204 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899625746} from checkpoint 204
09:27:10.724 [Async calls on Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (2/4)] INFO  o.a.f.s.a.f.s.TwoPhaseCommitSinkFunction - FlinkKafkaProducer011 1/4 - checkpoint 204 complete, committing transaction TransactionHolder{handle=KafkaTransactionState [transactionalId=null, producerId=-1, epoch=-1], transactionStartTime=1595899625745} from checkpoint 204
09:27:10.739 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.internal.Kafka09Fetcher - Committing offsets to Kafka failed. This does not compromise Flink's checkpoints.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
09:27:10.752 [Kafka 0.10 Fetcher for Source: Custom Source -> (Map -> Sink: Print to Std. Out, Sink: Unnamed) (3/4)] WARN  o.a.f.s.c.k.FlinkKafkaConsumerBase - Consumer subtask 2 failed async Kafka commit.
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1096)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getRecordsFromKafka(KafkaConsumerThread.java:539)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:267)
